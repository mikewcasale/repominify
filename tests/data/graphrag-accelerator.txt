This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2024-12-13T22:32:13.336Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repomix, visit: https://github.com/yamadashy/repomix

================================================================
Repository Structure
================================================================
backend/
  src/
    api/
      azure_clients.py
      common.py
      data.py
      graph.py
      index_configuration.py
      index.py
      query_streaming.py
      query.py
      source.py
    reporting/
      __init__.py
      application_insights_workflow_callbacks.py
      blob_workflow_callbacks.py
      console_workflow_callbacks.py
      load_reporter.py
      pipeline_job_workflow_callbacks.py
      reporter_singleton.py
      typing.py
    tests/
      api/
        test_azure_clients.py
      conftest.py
      test_all_index_endpoint.py
    typing/
      __init__.py
      pipeline.py
    utils/
      graph.py
      query.py
      workflows.py
    main.py
    models.py
  manage-indexing-jobs.py
  run-indexing-job.py

================================================================
Repository Files
================================================================

================
File: backend/src/api/azure_clients.py
================
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT License.

import os

from azure.cosmos import (
    ContainerProxy,
    CosmosClient,
    DatabaseProxy,
)
from azure.identity import DefaultAzureCredential
from azure.storage.blob import BlobServiceClient
from azure.storage.blob.aio import BlobServiceClient as BlobServiceClientAsync
from environs import Env

ENDPOINT_ERROR_MSG = "Could not find connection string in environment variables"

from dotenv import load_dotenv

load_dotenv()


class CosmosClientSingleton:
    _instance = None
    _env = Env()

    @classmethod
    def get_instance(cls):
        if cls._instance is None:
            endpoint = os.environ["COSMOS_URI_ENDPOINT"]
            credential = DefaultAzureCredential()
            cls._instance = CosmosClient(endpoint, credential)
        return cls._instance


class BlobServiceClientSingleton:
    _instance = None
    _env = Env()

    @classmethod
    def get_instance(cls):
        if cls._instance is None:
            account_url = os.environ["STORAGE_ACCOUNT_BLOB_URL"]
            credential = DefaultAzureCredential()
            cls._instance = BlobServiceClient(account_url, credential=credential)
        return cls._instance

    @classmethod
    def get_storage_account_name(cls):
        account_url = os.environ["STORAGE_ACCOUNT_BLOB_URL"]
        return account_url.split("//")[1].split(".")[0]


class BlobServiceClientSingletonAsync:
    _instance = None
    _env = Env()

    @classmethod
    def get_instance(cls):
        if cls._instance is None:
            account_url = os.environ["STORAGE_ACCOUNT_BLOB_URL"]
            credential = DefaultAzureCredential()
            cls._instance = BlobServiceClientAsync(account_url, credential=credential)
        return cls._instance

    @classmethod
    def get_storage_account_name(cls):
        account_url = os.environ["STORAGE_ACCOUNT_BLOB_URL"]
        return account_url.split("//")[1].split(".")[0]


def get_database_client(database_name: str) -> DatabaseProxy:
    client = CosmosClientSingleton.get_instance()
    return client.get_database_client(database_name)


def get_database_container_client(
    database_name: str, container_name: str
) -> ContainerProxy:
    db_client = get_database_client(database_name)
    return db_client.get_container_client(container_name)


class AzureStorageClientManager:
    """
    Manages the Azure storage clients for blob storage and Cosmos DB.

    Attributes:
        azure_storage_blob_url (str): The blob endpoint for azure storage.
        cosmos_uri_endpoint (str): The uri endpoint for the Cosmos DB.
        _blob_service_client (BlobServiceClient): The blob service client.
        _blob_service_client_async (BlobServiceClientAsync): The asynchronous blob service client.
        _cosmos_client (CosmosClient): The Cosmos DB client.
        _cosmos_database_client (DatabaseProxy): The Cosmos DB database client.
        _cosmos_container_client (ContainerProxy): The Cosmos DB container client.
    """

    def __init__(self) -> None:
        self._env = Env()
        self.azure_storage_blob_url = self._env.str(
            "STORAGE_ACCOUNT_BLOB_URL", ENDPOINT_ERROR_MSG
        )
        self.cosmos_uri_endpoint = self._env.str(
            "COSMOS_URI_ENDPOINT", ENDPOINT_ERROR_MSG
        )
        credential = DefaultAzureCredential()
        self._blob_service_client = BlobServiceClient(
            account_url=os.environ["STORAGE_ACCOUNT_BLOB_URL"], credential=credential
        )
        self._blob_service_client_async = BlobServiceClientAsync(
            account_url=os.environ["STORAGE_ACCOUNT_BLOB_URL"], credential=credential
        )
        self._cosmos_client = CosmosClient(
            url=os.environ["COSMOS_URI_ENDPOINT"], credential=credential
        )

    def get_blob_service_client(self) -> BlobServiceClient:
        """
        Returns the blob service client.

        Returns:
            BlobServiceClient: The blob service client.
        """
        return self._blob_service_client

    def get_blob_service_client_async(self) -> BlobServiceClientAsync:
        """
        Returns the asynchronous blob service client.

        Returns:
            BlobServiceClientAsync: The asynchronous blob service client.
        """
        return self._blob_service_client_async

    def get_cosmos_client(self) -> CosmosClient:
        """
        Returns the Cosmos DB client.

        Returns:
            CosmosClient: The Cosmos DB client.
        """
        return self._cosmos_client

    def get_cosmos_database_client(self, database_name: str) -> DatabaseProxy:
        """
        Returns the Cosmos DB database client.

        Args:
            database_name (str): The name of the database.

        Returns:
            DatabaseProxy: The Cosmos DB database client.
        """
        if not hasattr(self, "_cosmos_database_client"):
            self._cosmos_database_client = self._cosmos_client.get_database_client(
                database=database_name
            )
        return self._cosmos_database_client

    def get_cosmos_container_client(
        self, database_name: str, container_name: str
    ) -> ContainerProxy:
        """
        Returns the Cosmos DB container client.

        Args:
            database_name (str): The name of the database.
            container_name (str): The name of the container.

        Returns:
            ContainerProxy: The Cosmos DB container client.
        """
        if not hasattr(self, "_cosmos_container_client"):
            self._cosmos_container_client = self.get_cosmos_database_client(
                database_name=database_name
            ).get_container_client(container=container_name)
        return self._cosmos_container_client

================
File: backend/src/api/common.py
================
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT License.

import hashlib
import re

from fastapi import HTTPException

from src.api.azure_clients import (
    AzureStorageClientManager,
    BlobServiceClientSingleton,
)

blob_service_client = BlobServiceClientSingleton.get_instance()
azure_storage_client_manager = AzureStorageClientManager()


def delete_blob_container(container_name: str):
    """
    Delete a blob container. If it does not exist, do nothing.
    If exception is raised, the calling function should catch it.
    """
    if blob_service_client.get_container_client(container_name).exists():
        blob_service_client.delete_container(container_name)


def validate_index_file_exist(index_name: str, file_name: str):
    """
    Check if index exists and that the specified blob file exists.

    A "valid" index is defined by having an entry in the container-store table in cosmos db.
    Further checks are done to ensure the blob container and file exist.

    Args:
    -----
    index_name (str)
        Name of the index to validate.
    file_name (str)
        The blob file to be validated.

    Raises: ValueError
    """
    # verify index_name is a valid index by checking container-store in cosmos db
    try:
        container_store_client = (
            azure_storage_client_manager.get_cosmos_container_client(
                database_name="graphrag", container_name="container-store"
            )
        )
        container_store_client.read_item(index_name, index_name)
    except Exception:
        raise ValueError(f"Container {index_name} is not a valid index.")
    # check for file existence
    index_container_client = blob_service_client.get_container_client(index_name)
    if not index_container_client.exists():
        raise ValueError(f"Container {index_name} not found.")
    if not index_container_client.get_blob_client(file_name).exists():
        raise ValueError(f"File {file_name} in container {index_name} not found.")


def validate_blob_container_name(container_name: str):
    """
    Check if container name is valid based on Azure resource naming rules.

        - A blob container name must be between 3 and 63 characters in length.
        - Start with a letter or number
        - All letters used in blob container names must be lowercase.
        - Contain only letters, numbers, or the hyphen.
        - Consecutive hyphens are not permitted.
        - Cannot end with a hyphen.

    Args:
    -----
    container_name (str)
        The blob container name to be validated.

    Raises: ValueError
    """
    # Check the length of the name
    if len(container_name) < 3 or len(container_name) > 63:
        raise ValueError(
            f"Container name must be between 3 and 63 characters in length. Name provided was {len(container_name)} characters long."
        )

    # Check if the name starts with a letter or number
    if not container_name[0].isalnum():
        raise ValueError(
            f"Container name must start with a letter or number. Starting character was {container_name[0]}."
        )

    # Check for valid characters (letters, numbers, hyphen) and lowercase letters
    if not re.match("^[a-z0-9-]+$", container_name):
        raise ValueError(
            f"Container name must only contain:\n- lowercase letters\n- numbers\n- or hyphens\nName provided was {container_name}."
        )

    # Check for consecutive hyphens
    if "--" in container_name:
        raise ValueError(
            f"Container name cannot contain consecutive hyphens. Name provided was {container_name}."
        )

    # Check for hyphens at the end of the name
    if container_name[-1] == "-":
        raise ValueError(
            f"Container name cannot end with a hyphen. Name provided was {container_name}."
        )


def sanitize_name(name: str | None) -> str | None:
    """
    Sanitize a human-readable name by converting it to a SHA256 hash, then truncate
    to 128 bit length to ensure it is within the 63 character limit imposed by Azure Storage.

    The sanitized name will be used to identify container names in both Azure Storage and CosmosDB.

    Args:
    -----
    name (str)
        The name to be sanitized.

    Returns: str
        The sanitized name.
    """
    if not name:
        return None
    name = name.encode()
    name_hash = hashlib.sha256(name)
    truncated_hash = name_hash.digest()[:16]  # get the first 16 bytes (128 bits)
    return truncated_hash.hex()


def retrieve_original_blob_container_name(sanitized_name: str) -> str | None:
    """
    Retrieve the original human-readable container name of a sanitized blob name.

    Args:
    -----
    sanitized_name (str)
        The sanitized name to be converted back to the original name.

    Returns: str
        The original human-readable name.
    """
    try:
        container_store_client = (
            azure_storage_client_manager.get_cosmos_container_client(
                database_name="graphrag", container_name="container-store"
            )
        )
        for item in container_store_client.read_all_items():
            if item["id"] == sanitized_name:
                return item["human_readable_name"]
    except Exception:
        raise HTTPException(
            status_code=500, detail="Error retrieving original blob name."
        )
    return None


def retrieve_original_entity_config_name(sanitized_name: str) -> str | None:
    """
    Retrieve the original human-readable entity config name of a sanitized entity config name.

    Args:
    -----
    sanitized_name (str)
        The sanitized name to be converted back to the original name.

    Returns: str
        The original human-readable name.
    """
    try:
        container_store_client = (
            azure_storage_client_manager.get_cosmos_container_client(
                database_name="graphrag", container_name="entities"
            )
        )
        for item in container_store_client.read_all_items():
            if item["id"] == sanitized_name:
                return item["human_readable_name"]
    except Exception:
        raise HTTPException(
            status_code=500, detail="Error retrieving original entity config name."
        )
    return None

================
File: backend/src/api/data.py
================
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT License.

import asyncio
import re
from math import ceil
from typing import List

from azure.storage.blob import ContainerClient
from fastapi import (
    APIRouter,
    HTTPException,
    UploadFile,
)

from src.api.azure_clients import (
    AzureStorageClientManager,
    BlobServiceClientSingletonAsync,
)
from src.api.common import (
    delete_blob_container,
    sanitize_name,
    validate_blob_container_name,
)
from src.models import (
    BaseResponse,
    StorageNameList,
)
from src.reporting import ReporterSingleton

azure_storage_client_manager = AzureStorageClientManager()

data_route = APIRouter(
    prefix="/data",
    tags=["Data Management"],
)


@data_route.get(
    "",
    summary="Get all data storage containers.",
    response_model=StorageNameList,
    responses={200: {"model": StorageNameList}},
)
async def get_all_data_storage_containers():
    """
    Retrieve a list of all data storage containers.
    """
    items = []
    try:
        container_store_client = (
            azure_storage_client_manager.get_cosmos_container_client(
                database_name="graphrag", container_name="container-store"
            )
        )
        for item in container_store_client.read_all_items():
            if item["type"] == "data":
                items.append(item["human_readable_name"])
    except Exception:
        reporter = ReporterSingleton().get_instance()
        reporter.on_error("Error getting list of blob containers.")
        raise HTTPException(
            status_code=500, detail="Error getting list of blob containers."
        )
    return StorageNameList(storage_name=items)


async def upload_file_async(
    upload_file: UploadFile, container_client: ContainerClient, overwrite: bool = True
) -> None:
    """
    Asynchronously upload a file to the specified blob container.
    Silently ignore errors that occur when overwrite=False.
    """
    blob_client = container_client.get_blob_client(upload_file.filename)
    with upload_file.file as file_stream:
        try:
            await blob_client.upload_blob(file_stream, overwrite=overwrite)
        except Exception:
            pass


class Cleaner:
    def __init__(self, file):
        self.file = file
        self.name = file.name
        self.changes = 0

    def clean(self, val, replacement=""):
        # fmt: off
        _illegal_xml_chars_RE = re.compile(
            "[\x00-\x08\x0b\x0c\x0e-\x1F\uD800-\uDFFF\uFFFE\uFFFF]"
        )
        # fmt: on
        self.changes += len(_illegal_xml_chars_RE.findall(val))
        return _illegal_xml_chars_RE.sub(replacement, val)

    def read(self, n):
        return self.clean(self.file.read(n).decode()).encode(
            encoding="utf-8", errors="strict"
        )

    def name(self):
        return self.file.name

    def __enter__(self):
        return self

    def __exit__(self, *args):
        self.file.close()


@data_route.post(
    "",
    summary="Upload data to a data storage container",
    response_model=BaseResponse,
    responses={200: {"model": BaseResponse}},
)
async def upload_files(
    files: List[UploadFile], storage_name: str, overwrite: bool = True
):
    """
    Create a data storage container in Azure and upload files to it.

    Args:
        files (List[UploadFile]): A list of files to be uploaded.
        storage_name (str): The name of the Azure Blob Storage container to which files will be uploaded.
        overwrite (bool): Whether to overwrite existing files with the same name. Defaults to True. If False, files that already exist will be skipped.

    Returns:
        BaseResponse: An instance of the BaseResponse model with a status message indicating the result of the upload.

    Raises:
        HTTPException: If the container name is invalid or if any error occurs during the upload process.
    """
    reporter = ReporterSingleton().get_instance()
    sanitized_storage_name = sanitize_name(storage_name)
    # ensure container name follows Azure Blob Storage naming conventions
    try:
        validate_blob_container_name(sanitized_storage_name)
    except ValueError:
        raise HTTPException(
            status_code=500,
            detail=f"Invalid blob container name: '{storage_name}'. Please try a different name.",
        )
    try:
        blob_service_client = BlobServiceClientSingletonAsync.get_instance()
        container_client = blob_service_client.get_container_client(
            sanitized_storage_name
        )
        if not await container_client.exists():
            await container_client.create_container()

        # clean files - remove illegal XML characters
        files = [UploadFile(Cleaner(f.file), filename=f.filename) for f in files]

        # upload files in batches of 1000 to avoid exceeding Azure Storage API limits
        batch_size = 1000
        batches = ceil(len(files) / batch_size)
        for i in range(batches):
            batch_files = files[i * batch_size : (i + 1) * batch_size]
            tasks = [
                upload_file_async(file, container_client, overwrite)
                for file in batch_files
            ]
            await asyncio.gather(*tasks)
        # update container-store in cosmosDB since upload process was successful
        container_store_client = (
            azure_storage_client_manager.get_cosmos_container_client(
                database_name="graphrag", container_name="container-store"
            )
        )
        container_store_client.upsert_item({
            "id": sanitized_storage_name,
            "human_readable_name": storage_name,
            "type": "data",
        })
        return BaseResponse(status="File upload successful.")
    except Exception:
        reporter.on_error("Error uploading files.", details={"files": files})
        raise HTTPException(
            status_code=500,
            detail=f"Error uploading files to container '{storage_name}'.",
        )


@data_route.delete(
    "/{storage_name}",
    summary="Delete a data storage container",
    response_model=BaseResponse,
    responses={200: {"model": BaseResponse}},
)
async def delete_files(storage_name: str):
    """
    Delete a specified data storage container.
    """
    sanitized_storage_name = sanitize_name(storage_name)
    try:
        # delete container in Azure Storage
        delete_blob_container(sanitized_storage_name)
        # update container-store in cosmosDB
        container_store_client = (
            azure_storage_client_manager.get_cosmos_container_client(
                database_name="graphrag", container_name="container-store"
            )
        )
        container_store_client.delete_item(
            item=sanitized_storage_name,
            partition_key=sanitized_storage_name,
        )
    except Exception:
        reporter = ReporterSingleton().get_instance()
        reporter.on_error(
            f"Error deleting container {storage_name}.",
            details={"Container": storage_name},
        )
        raise HTTPException(
            status_code=500, detail=f"Error deleting container '{storage_name}'."
        )
    return BaseResponse(status="Success")

================
File: backend/src/api/graph.py
================
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT License.

from io import BytesIO

import networkx as nx
from fastapi import (
    APIRouter,
    HTTPException,
)
from fastapi.responses import StreamingResponse

from src.api.azure_clients import BlobServiceClientSingleton
from src.api.common import (
    sanitize_name,
    validate_index_file_exist,
)
from src.models import GraphDataResponse
from src.reporting import ReporterSingleton

blob_service_client = BlobServiceClientSingleton.get_instance()


graph_route = APIRouter(
    prefix="/graph",
    tags=["Graph Operations"],
)


@graph_route.get(
    "/graphml/{index_name}",
    summary="Retrieve a GraphML file of the knowledge graph",
    response_description="GraphML file successfully downloaded",
)
async def retrieve_graphml_file(index_name: str):
    # validate index_name and graphml file existence
    sanitized_index_name = sanitize_name(index_name)
    graphml_filename = "summarized_graph.graphml"
    blob_filepath = f"output/{graphml_filename}"  # expected file location of the graph based on the workflow
    validate_index_file_exist(sanitized_index_name, blob_filepath)
    try:
        blob_client = blob_service_client.get_blob_client(
            container=sanitized_index_name, blob=blob_filepath
        )
        blob_stream = blob_client.download_blob().chunks()
        return StreamingResponse(
            blob_stream,
            media_type="application/octet-stream",
            headers={"Content-Disposition": f"attachment; filename={graphml_filename}"},
        )
    except Exception:
        reporter = ReporterSingleton().get_instance()
        reporter.on_error("Could not retrieve graphml file")
        raise HTTPException(
            status_code=500,
            detail=f"Could not retrieve graphml file for index '{index_name}'.",
        )


@graph_route.get(
    "/stats/{index_name}",
    summary="Retrieve basic graph statistics, number of nodes and edges",
    response_model=GraphDataResponse,
    responses={200: {"model": GraphDataResponse}},
)
async def retrieve_graph_stats(index_name: str):
    # validate index_name and knowledge graph file existence
    sanitized_index_name = sanitize_name(index_name)
    graph_file = "output/summarized_graph.graphml"  # expected filename of the graph based on the indexing workflow
    validate_index_file_exist(sanitized_index_name, graph_file)
    try:
        storage_client = blob_service_client.get_container_client(sanitized_index_name)
        blob_data = storage_client.download_blob(graph_file).readall()
        bytes_io = BytesIO(blob_data)
        g = nx.read_graphml(bytes_io)
        return GraphDataResponse(nodes=len(g.nodes), edges=len(g.edges))
    except Exception:
        reporter = ReporterSingleton().get_instance()
        reporter.on_error("Could not retrieve graph data file")
        raise HTTPException(
            status_code=500,
            detail=f"Could not retrieve graph statistics for index '{index_name}'.",
        )

================
File: backend/src/api/index_configuration.py
================
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT License.

import inspect
import os
import shutil
import traceback

import yaml
from fastapi import (
    APIRouter,
    HTTPException,
)
from fastapi.responses import StreamingResponse
from graphrag.prompt_tune.cli import prompt_tune as generate_fine_tune_prompts

from src.api.azure_clients import (
    AzureStorageClientManager,
    BlobServiceClientSingleton,
)
from src.api.common import (
    sanitize_name,
)
from src.reporting import ReporterSingleton

azure_storage_client_manager = AzureStorageClientManager()
index_configuration_route = APIRouter(
    prefix="/index/config", tags=["Index Configuration"]
)


@index_configuration_route.get(
    "/prompts",
    summary="Generate graphrag prompts from user-provided data.",
    description="Generating custom prompts from user-provided data may take several minutes to run based on the amount of data used.",
)
async def generate_prompts(storage_name: str, limit: int = 5):
    """
    Automatically generate custom prompts for entity entraction,
    community reports, and summarize descriptions based on a sample of provided data.
    """
    # check for storage container existence
    blob_service_client = BlobServiceClientSingleton().get_instance()
    sanitized_storage_name = sanitize_name(storage_name)
    if not blob_service_client.get_container_client(sanitized_storage_name).exists():
        raise HTTPException(
            status_code=500,
            detail=f"Data container '{storage_name}' does not exist.",
        )
    this_directory = os.path.dirname(
        os.path.abspath(inspect.getfile(inspect.currentframe()))
    )

    # write custom settings.yaml to a file and store in a temporary directory
    data = yaml.safe_load(open(f"{this_directory}/pipeline-settings.yaml"))
    data["input"]["container_name"] = sanitized_storage_name
    temp_dir = f"/tmp/{sanitized_storage_name}_prompt_tuning"
    shutil.rmtree(temp_dir, ignore_errors=True)
    os.makedirs(temp_dir, exist_ok=True)
    with open(f"{temp_dir}/settings.yaml", "w") as f:
        yaml.dump(data, f, default_flow_style=False)

    # generate prompts
    try:
        await generate_fine_tune_prompts(
            config=f"{temp_dir}/settings.yaml",
            root=temp_dir,
            domain="",
            selection_method="random",
            limit=limit,
            skip_entity_types=True,
            output=f"{temp_dir}/prompts",
        )
    except Exception as e:
        reporter = ReporterSingleton().get_instance()
        error_details = {
            "storage_name": storage_name,
        }
        reporter.on_error(
            message="Auto-prompt generation failed.",
            cause=e,
            stack=traceback.format_exc(),
            details=error_details,
        )
        raise HTTPException(
            status_code=500,
            detail=f"Error generating prompts for data in '{storage_name}'. Please try a lower limit.",
        )

    # zip up the generated prompt files and return the zip file
    temp_archive = (
        f"{temp_dir}/prompts"  # will become a zip file with the name prompts.zip
    )
    shutil.make_archive(temp_archive, "zip", root_dir=temp_dir, base_dir="prompts")

    def iterfile(file_path: str):
        with open(file_path, mode="rb") as file_like:
            yield from file_like

    return StreamingResponse(iterfile(f"{temp_archive}.zip"))

================
File: backend/src/api/index.py
================
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT License.

import asyncio
import inspect
import os
import traceback
from time import time
from typing import cast

import yaml
from azure.identity import DefaultAzureCredential
from azure.search.documents.indexes import SearchIndexClient
from datashaper import WorkflowCallbacksManager
from fastapi import (
    APIRouter,
    HTTPException,
    UploadFile,
)
from graphrag.config import create_graphrag_config
from graphrag.index import create_pipeline_config
from graphrag.index.bootstrap import bootstrap
from graphrag.index.run import run_pipeline_with_config
from kubernetes import (
    client,
    config,
)

from src.api.azure_clients import (
    AzureStorageClientManager,
    BlobServiceClientSingleton,
    get_database_container_client,
)
from src.api.common import (
    delete_blob_container,
    sanitize_name,
    validate_blob_container_name,
)
from src.models import (
    BaseResponse,
    IndexNameList,
    IndexStatusResponse,
    PipelineJob,
)
from src.reporting import ReporterSingleton
from src.reporting.load_reporter import load_pipeline_reporter
from src.reporting.pipeline_job_workflow_callbacks import PipelineJobWorkflowCallbacks
from src.reporting.typing import Reporters
from src.typing import PipelineJobState

blob_service_client = BlobServiceClientSingleton.get_instance()
azure_storage_client_manager = (
    AzureStorageClientManager()
)  # TODO: update API to use the AzureStorageClientManager

ai_search_url = os.environ["AI_SEARCH_URL"]
ai_search_audience = os.environ["AI_SEARCH_AUDIENCE"]

index_route = APIRouter(
    prefix="/index",
    tags=["Index Operations"],
)


@index_route.post(
    "",
    summary="Build an index",
    response_model=BaseResponse,
    responses={200: {"model": BaseResponse}},
)
async def setup_indexing_pipeline(
    storage_name: str,
    index_name: str,
    entity_extraction_prompt: UploadFile | None = None,
    community_report_prompt: UploadFile | None = None,
    summarize_descriptions_prompt: UploadFile | None = None,
):
    _blob_service_client = BlobServiceClientSingleton().get_instance()
    pipelinejob = PipelineJob()

    # validate index name against blob container naming rules
    sanitized_index_name = sanitize_name(index_name)
    try:
        validate_blob_container_name(sanitized_index_name)
    except ValueError:
        raise HTTPException(
            status_code=500,
            detail=f"Invalid index name: {index_name}",
        )

    # check for data container existence
    sanitized_storage_name = sanitize_name(storage_name)
    if not _blob_service_client.get_container_client(sanitized_storage_name).exists():
        raise HTTPException(
            status_code=500,
            detail=f"Storage blob container {storage_name} does not exist",
        )

    # check for prompts
    entity_extraction_prompt_content = (
        entity_extraction_prompt.file.read().decode("utf-8")
        if entity_extraction_prompt
        else None
    )
    community_report_prompt_content = (
        community_report_prompt.file.read().decode("utf-8")
        if community_report_prompt
        else None
    )
    summarize_descriptions_prompt_content = (
        summarize_descriptions_prompt.file.read().decode("utf-8")
        if summarize_descriptions_prompt
        else None
    )

    # check for existing index job
    # it is okay if job doesn't exist, but if it does,
    # it must not be scheduled or running
    if pipelinejob.item_exist(sanitized_index_name):
        existing_job = pipelinejob.load_item(sanitized_index_name)
        if (PipelineJobState(existing_job.status) == PipelineJobState.SCHEDULED) or (
            PipelineJobState(existing_job.status) == PipelineJobState.RUNNING
        ):
            raise HTTPException(
                status_code=202,  # request has been accepted for processing but is not complete.
                detail=f"Index '{index_name}' already exists and has not finished building.",
            )
        # if indexing job is in a failed state, delete the associated K8s job and pod to allow for a new job to be scheduled
        if PipelineJobState(existing_job.status) == PipelineJobState.FAILED:
            _delete_k8s_job(
                f"indexing-job-{sanitized_index_name}", os.environ["AKS_NAMESPACE"]
            )
        # reset the pipeline job details
        existing_job._status = PipelineJobState.SCHEDULED
        existing_job._percent_complete = 0
        existing_job._progress = ""
        existing_job._all_workflows = existing_job._completed_workflows = (
            existing_job._failed_workflows
        ) = []
        existing_job._entity_extraction_prompt = entity_extraction_prompt_content
        existing_job._community_report_prompt = community_report_prompt_content
        existing_job._summarize_descriptions_prompt = (
            summarize_descriptions_prompt_content
        )
        existing_job._epoch_request_time = int(time())
        existing_job.update_db()
    else:
        pipelinejob.create_item(
            id=sanitized_index_name,
            human_readable_index_name=index_name,
            human_readable_storage_name=storage_name,
            entity_extraction_prompt=entity_extraction_prompt_content,
            community_report_prompt=community_report_prompt_content,
            summarize_descriptions_prompt=summarize_descriptions_prompt_content,
            status=PipelineJobState.SCHEDULED,
        )

    return BaseResponse(status="Indexing job scheduled")


async def _start_indexing_pipeline(index_name: str):
    # get sanitized name
    sanitized_index_name = sanitize_name(index_name)

    # update or create new item in container-store in cosmosDB
    _blob_service_client = BlobServiceClientSingleton().get_instance()
    if not _blob_service_client.get_container_client(sanitized_index_name).exists():
        _blob_service_client.create_container(sanitized_index_name)
    container_store_client = get_database_container_client(
        database_name="graphrag", container_name="container-store"
    )
    container_store_client.upsert_item({
        "id": sanitized_index_name,
        "human_readable_name": index_name,
        "type": "index",
    })

    reporter = ReporterSingleton().get_instance()
    pipelinejob = PipelineJob()
    pipeline_job = pipelinejob.load_item(sanitized_index_name)
    sanitized_storage_name = pipeline_job.sanitized_storage_name
    storage_name = pipeline_job.human_readable_index_name

    # download nltk dependencies
    bootstrap()

    # load custom pipeline settings
    this_directory = os.path.dirname(
        os.path.abspath(inspect.getfile(inspect.currentframe()))
    )
    data = yaml.safe_load(open(f"{this_directory}/pipeline-settings.yaml"))
    # dynamically set some values
    data["input"]["container_name"] = sanitized_storage_name
    data["storage"]["container_name"] = sanitized_index_name
    data["reporting"]["container_name"] = sanitized_index_name
    data["cache"]["container_name"] = sanitized_index_name
    if "vector_store" in data["embeddings"]:
        data["embeddings"]["vector_store"]["collection_name"] = (
            f"{sanitized_index_name}_description_embedding"
        )

    # set prompts for entity extraction, community report, and summarize descriptions.
    if pipeline_job.entity_extraction_prompt:
        fname = "entity-extraction-prompt.txt"
        with open(fname, "w") as outfile:
            outfile.write(pipeline_job.entity_extraction_prompt)
        data["entity_extraction"]["prompt"] = fname
    else:
        data.pop("entity_extraction")
    if pipeline_job.community_report_prompt:
        fname = "community-report-prompt.txt"
        with open(fname, "w") as outfile:
            outfile.write(pipeline_job.community_report_prompt)
        data["community_reports"]["prompt"] = fname
    else:
        data.pop("community_reports")
    if pipeline_job.summarize_descriptions_prompt:
        fname = "summarize-descriptions-prompt.txt"
        with open(fname, "w") as outfile:
            outfile.write(pipeline_job.summarize_descriptions_prompt)
        data["summarize_descriptions"]["prompt"] = fname
    else:
        data.pop("summarize_descriptions")

    # generate the default pipeline and override with custom settings
    parameters = create_graphrag_config(data, ".")
    pipeline_config = create_pipeline_config(parameters, True)

    # reset pipeline job details
    pipeline_job.status = PipelineJobState.RUNNING
    pipeline_job.all_workflows = []
    pipeline_job.completed_workflows = []
    pipeline_job.failed_workflows = []
    for workflow in pipeline_config.workflows:
        pipeline_job.all_workflows.append(workflow.name)

    # create new reporters/callbacks just for this job
    reporters = []
    reporter_names = os.getenv("REPORTERS", Reporters.CONSOLE.name.upper()).split(",")
    for reporter_name in reporter_names:
        try:
            reporters.append(Reporters[reporter_name.upper()])
        except KeyError:
            raise ValueError(f"Unknown reporter type: {reporter_name}")
    workflow_callbacks = load_pipeline_reporter(
        index_name=index_name,
        num_workflow_steps=len(pipeline_job.all_workflows),
        reporting_dir=sanitized_index_name,
        reporters=reporters,
    )

    # add pipeline job callback to the callback manager
    cast(WorkflowCallbacksManager, workflow_callbacks).register(
        PipelineJobWorkflowCallbacks(pipeline_job)
    )

    # run the pipeline
    try:
        async for workflow_result in run_pipeline_with_config(
            config_or_path=pipeline_config,
            callbacks=workflow_callbacks,
            progress_reporter=None,
        ):
            await asyncio.sleep(0)
            if len(workflow_result.errors or []) > 0:
                # if the workflow failed, record the failure
                pipeline_job.failed_workflows.append(workflow_result.workflow)
                pipeline_job.update_db()
                # TODO: exit early if a workflow fails and add more detailed error logging

        # if job is done, check if any workflow steps failed
        if len(pipeline_job.failed_workflows) > 0:
            pipeline_job.status = PipelineJobState.FAILED
        else:
            # record the workflow completion
            pipeline_job.status = PipelineJobState.COMPLETE
            pipeline_job.percent_complete = 100

        pipeline_job.progress = (
            f"{len(pipeline_job.completed_workflows)} out of "
            f"{len(pipeline_job.all_workflows)} workflows completed successfully."
        )

        workflow_callbacks.on_log(
            message=f"Indexing pipeline complete for index'{index_name}'.",
            details={
                "index": index_name,
                "storage_name": storage_name,
                "status_message": "indexing pipeline complete",
            },
        )

        del workflow_callbacks  # garbage collect
        if pipeline_job.status == PipelineJobState.FAILED:
            exit(1)  # signal to AKS that indexing job failed

    except Exception as e:
        pipeline_job.status = PipelineJobState.FAILED

        # update failed state in cosmos db
        error_details = {
            "index": index_name,
            "storage_name": storage_name,
        }
        # log error in local index directory logs
        workflow_callbacks.on_error(
            message=f"Indexing pipeline failed for index '{index_name}'.",
            cause=e,
            stack=traceback.format_exc(),
            details=error_details,
        )
        # log error in global index directory logs
        reporter.on_error(
            message=f"Indexing pipeline failed for index '{index_name}'.",
            cause=e,
            stack=traceback.format_exc(),
            details=error_details,
        )
        raise HTTPException(
            status_code=500,
            detail=f"Error encountered during indexing job for index '{index_name}'.",
        )


@index_route.get(
    "",
    summary="Get all indexes",
    response_model=IndexNameList,
    responses={200: {"model": IndexNameList}},
)
async def get_all_indexes():
    """
    Retrieve a list of all index names.
    """
    items = []
    try:
        container_store_client = get_database_container_client(
            database_name="graphrag", container_name="container-store"
        )
        for item in container_store_client.read_all_items():
            if item["type"] == "index":
                items.append(item["human_readable_name"])
    except Exception:
        reporter = ReporterSingleton().get_instance()
        reporter.on_error("Error retrieving index names")
    return IndexNameList(index_name=items)


def _get_pod_name(job_name: str, namespace: str) -> str | None:
    """Retrieve the name of a kubernetes pod associated with a given job name."""
    # function should work only when running in AKS
    if not os.getenv("KUBERNETES_SERVICE_HOST"):
        return None
    config.load_incluster_config()
    v1 = client.CoreV1Api()
    ret = v1.list_namespaced_pod(namespace=namespace)
    for i in ret.items:
        if job_name in i.metadata.name:
            return i.metadata.name
    return None


def _delete_k8s_job(job_name: str, namespace: str) -> None:
    """Delete a kubernetes job.
    Must delete K8s job first and then any pods associated with it
    """
    # function should only work when running in AKS
    if not os.getenv("KUBERNETES_SERVICE_HOST"):
        return None
    reporter = ReporterSingleton().get_instance()
    config.load_incluster_config()
    try:
        batch_v1 = client.BatchV1Api()
        batch_v1.delete_namespaced_job(name=job_name, namespace=namespace)
    except Exception:
        reporter.on_error(
            message=f"Error deleting k8s job {job_name}.",
            details={"container": job_name},
        )
        pass
    try:
        core_v1 = client.CoreV1Api()
        job_pod = _get_pod_name(job_name, os.environ["AKS_NAMESPACE"])
        if job_pod:
            core_v1.delete_namespaced_pod(job_pod, namespace=namespace)
    except Exception:
        reporter.on_error(
            message=f"Error deleting k8s pod for job {job_name}.",
            details={"container": job_name},
        )
        pass


@index_route.delete(
    "/{index_name}",
    summary="Delete a specified index",
    response_model=BaseResponse,
    responses={200: {"model": BaseResponse}},
)
async def delete_index(index_name: str):
    """
    Delete a specified index.
    """
    sanitized_index_name = sanitize_name(index_name)
    try:
        # kill indexing job if it is running
        if os.getenv("KUBERNETES_SERVICE_HOST"):  # only found if in AKS
            _delete_k8s_job(f"indexing-job-{sanitized_index_name}", "graphrag")

        # remove blob container and all associated entries in cosmos db
        try:
            delete_blob_container(sanitized_index_name)
        except Exception:
            pass

        # update container-store in cosmosDB
        try:
            container_store_client = get_database_container_client(
                database_name="graphrag", container_name="container-store"
            )
            container_store_client.delete_item(
                item=sanitized_index_name, partition_key=sanitized_index_name
            )
        except Exception:
            pass

        # update jobs database in cosmosDB
        try:
            jobs_container = get_database_container_client(
                database_name="graphrag", container_name="jobs"
            )
            jobs_container.delete_item(
                item=sanitized_index_name, partition_key=sanitized_index_name
            )
        except Exception:
            pass

        index_client = SearchIndexClient(
            endpoint=ai_search_url,
            credential=DefaultAzureCredential(),
            audience=ai_search_audience,
        )
        ai_search_index_name = f"{sanitized_index_name}_description_embedding"
        if ai_search_index_name in index_client.list_index_names():
            index_client.delete_index(ai_search_index_name)

    except Exception:
        reporter = ReporterSingleton().get_instance()
        reporter.on_error(
            message=f"Error encountered while deleting all data for index {index_name}.",
            stack=None,
            details={"container": index_name},
        )
        raise HTTPException(
            status_code=500, detail=f"Error deleting index '{index_name}'."
        )

    return BaseResponse(status="Success")


@index_route.get(
    "/status/{index_name}",
    summary="Track the status of an indexing job",
    response_model=IndexStatusResponse,
)
async def get_index_job_status(index_name: str):
    pipelinejob = PipelineJob()  # TODO: fix class so initiliazation is not required
    sanitized_index_name = sanitize_name(index_name)
    if pipelinejob.item_exist(sanitized_index_name):
        pipeline_job = pipelinejob.load_item(sanitized_index_name)
        return IndexStatusResponse(
            status_code=200,
            index_name=pipeline_job.human_readable_index_name,
            storage_name=pipeline_job.human_readable_storage_name,
            status=pipeline_job.status.value,
            percent_complete=pipeline_job.percent_complete,
            progress=pipeline_job.progress,
        )
    raise HTTPException(status_code=404, detail=f"Index '{index_name}' does not exist.")

================
File: backend/src/api/query_streaming.py
================
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT License.

import inspect
import json
import os
import traceback

import pandas as pd
import yaml
from fastapi import (
    APIRouter,
    HTTPException,
)
from fastapi.responses import StreamingResponse
from graphrag.config import create_graphrag_config
from graphrag.query.api import (
    global_search_streaming as global_search_streaming_internal,
)
from graphrag.query.api import local_search_streaming as local_search_streaming_internal

from src.api.azure_clients import BlobServiceClientSingleton
from src.api.common import (
    sanitize_name,
    validate_index_file_exist,
)
from src.api.query import _is_index_complete
from src.models import GraphRequest
from src.reporting import ReporterSingleton
from src.utils import query as query_helper

from .query import _get_embedding_description_store, _update_context

query_streaming_route = APIRouter(
    prefix="/query/streaming",
    tags=["Query Streaming Operations"],
)


@query_streaming_route.post(
    "/global",
    summary="Stream a response back after performing a global search",
    description="The global query method generates answers by searching over all AI-generated community reports in a map-reduce fashion. This is a resource-intensive method, but often gives good responses for questions that require an understanding of the dataset as a whole.",
)
async def global_search_streaming(request: GraphRequest):
    # this is a slightly modified version of src.api.query.global_query() method
    if isinstance(request.index_name, str):
        index_names = [request.index_name]
    else:
        index_names = request.index_name
    sanitized_index_names = [sanitize_name(name) for name in index_names]
    sanitized_index_names_link = {
        s: i for s, i in zip(sanitized_index_names, index_names)
    }

    for index_name in sanitized_index_names:
        if not _is_index_complete(index_name):
            raise HTTPException(
                status_code=500,
                detail=f"{sanitized_index_names_link[index_name]} not ready for querying.",
            )

    COMMUNITY_REPORT_TABLE = "output/create_final_community_reports.parquet"
    ENTITIES_TABLE = "output/create_final_entities.parquet"
    NODES_TABLE = "output/create_final_nodes.parquet"

    if isinstance(request.community_level, int):
        COMMUNITY_LEVEL = request.community_level
    else:
        # Current investigations show that community level 1 is the most useful for global search. Set this as the default value
        COMMUNITY_LEVEL = 1

    for index_name in sanitized_index_names:
        validate_index_file_exist(index_name, COMMUNITY_REPORT_TABLE)
        validate_index_file_exist(index_name, ENTITIES_TABLE)
        validate_index_file_exist(index_name, NODES_TABLE)
    try:
        links = {
            "nodes": {},
            "community": {},
            "entities": {},
            "text_units": {},
            "relationships": {},
            "covariates": {},
        }
        max_vals = {
            "nodes": -1,
            "community": -1,
            "entities": -1,
            "text_units": -1,
            "relationships": -1,
            "covariates": -1,
        }

        community_dfs = []
        entities_dfs = []
        nodes_dfs = []

        for index_name in sanitized_index_names:
            community_report_table_path = (
                f"abfs://{index_name}/{COMMUNITY_REPORT_TABLE}"
            )
            entities_table_path = f"abfs://{index_name}/{ENTITIES_TABLE}"
            nodes_table_path = f"abfs://{index_name}/{NODES_TABLE}"

            # read parquet files into DataFrames and add provenance information

            # note that nodes need to set before communities to that max community id makes sense
            nodes_df = query_helper.get_df(nodes_table_path)
            for i in nodes_df["human_readable_id"]:
                links["nodes"][i + max_vals["nodes"] + 1] = {
                    "index_name": sanitized_index_names_link[index_name],
                    "id": i,
                }
            if max_vals["nodes"] != -1:
                nodes_df["human_readable_id"] += max_vals["nodes"] + 1
            nodes_df["community"] = nodes_df["community"].apply(
                lambda x: str(int(x) + max_vals["community"] + 1) if x else x
            )
            nodes_df["title"] = nodes_df["title"].apply(lambda x: x + f"-{index_name}")
            nodes_df["source_id"] = nodes_df["source_id"].apply(
                lambda x: ",".join([i + f"-{index_name}" for i in x.split(",")])
            )
            max_vals["nodes"] = nodes_df["human_readable_id"].max()
            nodes_dfs.append(nodes_df)

            community_df = query_helper.get_df(community_report_table_path)
            for i in community_df["community"].astype(int):
                links["community"][i + max_vals["community"] + 1] = {
                    "index_name": sanitized_index_names_link[index_name],
                    "id": str(i),
                }
            if max_vals["community"] != -1:
                col = community_df["community"].astype(int) + max_vals["community"] + 1
                community_df["community"] = col.astype(str)
            max_vals["community"] = community_df["community"].astype(int).max()
            community_dfs.append(community_df)

            entities_df = query_helper.get_df(entities_table_path)
            for i in entities_df["human_readable_id"]:
                links["entities"][i + max_vals["entities"] + 1] = {
                    "index_name": sanitized_index_names_link[index_name],
                    "id": i,
                }
            if max_vals["entities"] != -1:
                entities_df["human_readable_id"] += max_vals["entities"] + 1
            entities_df["name"] = entities_df["name"].apply(
                lambda x: x + f"-{index_name}"
            )
            entities_df["text_unit_ids"] = entities_df["text_unit_ids"].apply(
                lambda x: [i + f"-{index_name}" for i in x]
            )
            max_vals["entities"] = entities_df["human_readable_id"].max()
            entities_dfs.append(entities_df)

        # merge the dataframes
        nodes_combined = pd.concat(nodes_dfs, axis=0, ignore_index=True, sort=False)
        community_combined = pd.concat(
            community_dfs, axis=0, ignore_index=True, sort=False
        )
        entities_combined = pd.concat(
            entities_dfs, axis=0, ignore_index=True, sort=False
        )

        # load custom pipeline settings
        this_directory = os.path.dirname(
            os.path.abspath(inspect.getfile(inspect.currentframe()))
        )
        data = yaml.safe_load(open(f"{this_directory}/pipeline-settings.yaml"))
        # layer the custom settings on top of the default configuration settings of graphrag
        parameters = create_graphrag_config(data, ".")

        return StreamingResponse(
            _wrapper(
                global_search_streaming_internal(
                    config=parameters,
                    nodes=nodes_combined,
                    entities=entities_combined,
                    community_reports=community_combined,
                    community_level=COMMUNITY_LEVEL,
                    response_type="Multiple Paragraphs",
                    query=request.query,
                ),
                links,
            ),
            media_type="application/json",
        )
    except Exception as e:
        reporter = ReporterSingleton().get_instance()
        reporter.on_error(
            message="Error encountered while streaming global search response",
            cause=e,
            stack=traceback.format_exc(),
        )
        raise HTTPException(status_code=500, detail=None)


@query_streaming_route.post(
    "/local",
    summary="Stream a response back after performing a local search",
    description="The local query method generates answers by combining relevant data from the AI-extracted knowledge-graph with text chunks of the raw documents. This method is suitable for questions that require an understanding of specific entities mentioned in the documents (e.g. What are the healing properties of chamomile?).",
)
async def local_search_streaming(request: GraphRequest):
    # this is a slightly modified version of src.api.query.local_query() method
    if isinstance(request.index_name, str):
        index_names = [request.index_name]
    else:
        index_names = request.index_name
    sanitized_index_names = [sanitize_name(name) for name in index_names]
    sanitized_index_names_link = {
        s: i for s, i in zip(sanitized_index_names, index_names)
    }

    for index_name in sanitized_index_names:
        if not _is_index_complete(index_name):
            raise HTTPException(
                status_code=500,
                detail=f"{sanitized_index_names_link[index_name]} not ready for querying.",
            )

    blob_service_client = BlobServiceClientSingleton.get_instance()

    community_dfs = []
    covariates_dfs = []
    entities_dfs = []
    nodes_dfs = []
    relationships_dfs = []
    text_units_dfs = []
    links = {
        "nodes": {},
        "community": {},
        "entities": {},
        "text_units": {},
        "relationships": {},
        "covariates": {},
    }
    max_vals = {
        "nodes": -1,
        "community": -1,
        "entities": -1,
        "text_units": -1,
        "relationships": -1,
        "covariates": -1,
    }

    COMMUNITY_REPORT_TABLE = "output/create_final_community_reports.parquet"
    COVARIATES_TABLE = "output/create_final_covariates.parquet"
    ENTITIES_TABLE = "output/create_final_entities.parquet"
    NODES_TABLE = "output/create_final_nodes.parquet"
    RELATIONSHIPS_TABLE = "output/create_final_relationships.parquet"
    TEXT_UNITS_TABLE = "output/create_final_text_units.parquet"

    if isinstance(request.community_level, int):
        COMMUNITY_LEVEL = request.community_level
    else:
        # Current investigations show that community level 2 is the most useful for local search. Set this as the default value
        COMMUNITY_LEVEL = 2

    try:
        for index_name in sanitized_index_names:
            # check for existence of files the query relies on to validate the index is complete
            validate_index_file_exist(index_name, COMMUNITY_REPORT_TABLE)
            validate_index_file_exist(index_name, ENTITIES_TABLE)
            validate_index_file_exist(index_name, NODES_TABLE)
            validate_index_file_exist(index_name, RELATIONSHIPS_TABLE)
            validate_index_file_exist(index_name, TEXT_UNITS_TABLE)

            community_report_table_path = (
                f"abfs://{index_name}/{COMMUNITY_REPORT_TABLE}"
            )
            covariates_table_path = f"abfs://{index_name}/{COVARIATES_TABLE}"
            entities_table_path = f"abfs://{index_name}/{ENTITIES_TABLE}"
            nodes_table_path = f"abfs://{index_name}/{NODES_TABLE}"
            relationships_table_path = f"abfs://{index_name}/{RELATIONSHIPS_TABLE}"
            text_units_table_path = f"abfs://{index_name}/{TEXT_UNITS_TABLE}"

            # read the parquet files into DataFrames and add provenance information

            # note that nodes need to set before communities to that max community id makes sense
            nodes_df = query_helper.get_df(nodes_table_path)
            for i in nodes_df["human_readable_id"]:
                links["nodes"][i + max_vals["nodes"] + 1] = {
                    "index_name": sanitized_index_names_link[index_name],
                    "id": i,
                }
            if max_vals["nodes"] != -1:
                nodes_df["human_readable_id"] += max_vals["nodes"] + 1
            nodes_df["community"] = nodes_df["community"].apply(
                lambda x: str(int(x) + max_vals["community"] + 1) if x else x
            )
            nodes_df["id"] = nodes_df["id"].apply(lambda x: x + f"-{index_name}")
            nodes_df["title"] = nodes_df["title"].apply(lambda x: x + f"-{index_name}")
            nodes_df["source_id"] = nodes_df["source_id"].apply(
                lambda x: ",".join([i + f"-{index_name}" for i in x.split(",")])
            )
            max_vals["nodes"] = nodes_df["human_readable_id"].max()
            nodes_dfs.append(nodes_df)

            community_df = query_helper.get_df(community_report_table_path)
            for i in community_df["community"].astype(int):
                links["community"][i + max_vals["community"] + 1] = {
                    "index_name": sanitized_index_names_link[index_name],
                    "id": str(i),
                }
            if max_vals["community"] != -1:
                col = community_df["community"].astype(int) + max_vals["community"] + 1
                community_df["community"] = col.astype(str)
            max_vals["community"] = community_df["community"].astype(int).max()
            community_dfs.append(community_df)

            entities_df = query_helper.get_df(entities_table_path)
            for i in entities_df["human_readable_id"]:
                links["entities"][i + max_vals["entities"] + 1] = {
                    "index_name": sanitized_index_names_link[index_name],
                    "id": i,
                }
            if max_vals["entities"] != -1:
                entities_df["human_readable_id"] += max_vals["entities"] + 1
            entities_df["id"] = entities_df["id"].apply(lambda x: x + f"-{index_name}")
            entities_df["name"] = entities_df["name"].apply(
                lambda x: x + f"-{index_name}"
            )
            entities_df["text_unit_ids"] = entities_df["text_unit_ids"].apply(
                lambda x: [i + f"-{index_name}" for i in x]
            )
            max_vals["entities"] = entities_df["human_readable_id"].max()
            entities_dfs.append(entities_df)

            relationships_df = query_helper.get_df(relationships_table_path)
            for i in relationships_df["human_readable_id"].astype(int):
                links["relationships"][i + max_vals["relationships"] + 1] = {
                    "index_name": sanitized_index_names_link[index_name],
                    "id": i,
                }
            if max_vals["relationships"] != -1:
                col = (
                    relationships_df["human_readable_id"].astype(int)
                    + max_vals["relationships"]
                    + 1
                )
                relationships_df["human_readable_id"] = col.astype(str)
            relationships_df["source"] = relationships_df["source"].apply(
                lambda x: x + f"-{index_name}"
            )
            relationships_df["target"] = relationships_df["target"].apply(
                lambda x: x + f"-{index_name}"
            )
            relationships_df["text_unit_ids"] = relationships_df["text_unit_ids"].apply(
                lambda x: [i + f"-{index_name}" for i in x]
            )
            max_vals["relationships"] = (
                relationships_df["human_readable_id"].astype(int).max()
            )
            relationships_dfs.append(relationships_df)

            text_units_df = query_helper.get_df(text_units_table_path)
            text_units_df["id"] = text_units_df["id"].apply(
                lambda x: f"{x}-{index_name}"
            )
            text_units_dfs.append(text_units_df)

            index_container_client = blob_service_client.get_container_client(
                index_name
            )
            if index_container_client.get_blob_client(COVARIATES_TABLE).exists():
                covariates_df = query_helper.get_df(covariates_table_path)
                if i in covariates_df["human_readable_id"].astype(int):
                    links["covariates"][i + max_vals["covariates"] + 1] = {
                        "index_name": sanitized_index_names_link[index_name],
                        "id": i,
                    }
                if max_vals["covariates"] != -1:
                    col = (
                        covariates_df["human_readable_id"].astype(int)
                        + max_vals["covariates"]
                        + 1
                    )
                    covariates_df["human_readable_id"] = col.astype(str)
                max_vals["covariates"] = (
                    covariates_df["human_readable_id"].astype(int).max()
                )
                covariates_dfs.append(covariates_df)

        nodes_combined = pd.concat(nodes_dfs, axis=0, ignore_index=True)
        community_combined = pd.concat(community_dfs, axis=0, ignore_index=True)
        entities_combined = pd.concat(entities_dfs, axis=0, ignore_index=True)
        text_units_combined = pd.concat(text_units_dfs, axis=0, ignore_index=True)
        relationships_combined = pd.concat(relationships_dfs, axis=0, ignore_index=True)
        covariates_combined = (
            pd.concat(covariates_dfs, axis=0, ignore_index=True)
            if covariates_dfs != []
            else None
        )

        # load custom pipeline settings
        this_directory = os.path.dirname(
            os.path.abspath(inspect.getfile(inspect.currentframe()))
        )
        data = yaml.safe_load(open(f"{this_directory}/pipeline-settings.yaml"))
        # layer the custom settings on top of the default configuration settings of graphrag
        parameters = create_graphrag_config(data, ".")

        # add index_names to vector_store args
        parameters.embeddings.vector_store["index_names"] = sanitized_index_names
        # internally write over the get_embedding_description_store
        # method to use the multi-index collection.
        import graphrag.query.api

        graphrag.query.api._get_embedding_description_store = (
            _get_embedding_description_store
        )

        # perform streaming local search
        return StreamingResponse(
            _wrapper(
                local_search_streaming_internal(
                    config=parameters,
                    nodes=nodes_combined,
                    entities=entities_combined,
                    community_reports=community_combined,
                    text_units=text_units_combined,
                    relationships=relationships_combined,
                    covariates=covariates_combined,
                    community_level=COMMUNITY_LEVEL,
                    response_type="Multiple Paragraphs",
                    query=request.query,
                ),
                links,
            ),
            media_type="application/json",
        )
    except Exception as e:
        reporter = ReporterSingleton().get_instance()
        reporter.on_error(
            message="Error encountered while streaming local search response",
            cause=e,
            stack=traceback.format_exc(),
        )
        raise HTTPException(status_code=500, detail=None)


async def _wrapper(x, links):
    context = None
    async for i in x:
        if context:
            yield json.dumps({"token": i, "context": None}).encode("utf-8") + b"\n"
        else:
            context = i
    context = _update_context(context, links)
    context = json.dumps({"token": "<EOM>", "context": context}).encode("utf-8") + b"\n"
    yield context

================
File: backend/src/api/query.py
================
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT License.

import inspect
import json
import os
import traceback
from typing import Any

import pandas as pd
import yaml
from azure.identity import DefaultAzureCredential
from azure.search.documents import SearchClient
from azure.search.documents.models import VectorizedQuery
from fastapi import (
    APIRouter,
    HTTPException,
)
from graphrag.config import create_graphrag_config
from graphrag.model.types import TextEmbedder
from graphrag.query.api import global_search, local_search
from graphrag.vector_stores.base import (
    BaseVectorStore,
    VectorStoreDocument,
    VectorStoreSearchResult,
)

from src.api.azure_clients import BlobServiceClientSingleton
from src.api.common import (
    sanitize_name,
    validate_index_file_exist,
)
from src.models import (
    GraphRequest,
    GraphResponse,
    PipelineJob,
)
from src.reporting import ReporterSingleton
from src.typing import PipelineJobState
from src.utils import query as query_helper

query_route = APIRouter(
    prefix="/query",
    tags=["Query Operations"],
)


@query_route.post(
    "/global",
    summary="Perform a global search across the knowledge graph index",
    description="The global query method generates answers by searching over all AI-generated community reports in a map-reduce fashion. This is a resource-intensive method, but often gives good responses for questions that require an understanding of the dataset as a whole.",
    response_model=GraphResponse,
    responses={200: {"model": GraphResponse}},
)
async def global_query(request: GraphRequest):
    # this is a slightly modified version of the graphrag.query.cli.run_global_search method
    if isinstance(request.index_name, str):
        index_names = [request.index_name]
    else:
        index_names = request.index_name
    sanitized_index_names = [sanitize_name(name) for name in index_names]
    sanitized_index_names_link = {
        s: i for s, i in zip(sanitized_index_names, index_names)
    }

    for index_name in sanitized_index_names:
        if not _is_index_complete(index_name):
            raise HTTPException(
                status_code=500,
                detail=f"{index_name} not ready for querying.",
            )

    COMMUNITY_REPORT_TABLE = "output/create_final_community_reports.parquet"
    ENTITIES_TABLE = "output/create_final_entities.parquet"
    NODES_TABLE = "output/create_final_nodes.parquet"

    for index_name in sanitized_index_names:
        validate_index_file_exist(index_name, COMMUNITY_REPORT_TABLE)
        validate_index_file_exist(index_name, ENTITIES_TABLE)
        validate_index_file_exist(index_name, NODES_TABLE)

    if isinstance(request.community_level, int):
        COMMUNITY_LEVEL = request.community_level
    else:
        # Current investigations show that community level 1 is the most useful for global search. Set this as the default value
        COMMUNITY_LEVEL = 1

    try:
        links = {
            "nodes": {},
            "community": {},
            "entities": {},
            "text_units": {},
            "relationships": {},
            "covariates": {},
        }
        max_vals = {
            "nodes": -1,
            "community": -1,
            "entities": -1,
            "text_units": -1,
            "relationships": -1,
            "covariates": -1,
        }

        community_dfs = []
        entities_dfs = []
        nodes_dfs = []

        for index_name in sanitized_index_names:
            community_report_table_path = (
                f"abfs://{index_name}/{COMMUNITY_REPORT_TABLE}"
            )
            entities_table_path = f"abfs://{index_name}/{ENTITIES_TABLE}"
            nodes_table_path = f"abfs://{index_name}/{NODES_TABLE}"

            # read the parquet files into DataFrames and add provenance information

            # note that nodes need to set before communities to that max community id makes sense
            nodes_df = query_helper.get_df(nodes_table_path)
            for i in nodes_df["human_readable_id"]:
                links["nodes"][i + max_vals["nodes"] + 1] = {
                    "index_name": sanitized_index_names_link[index_name],
                    "id": i,
                }
            if max_vals["nodes"] != -1:
                nodes_df["human_readable_id"] += max_vals["nodes"] + 1
            nodes_df["community"] = nodes_df["community"].apply(
                lambda x: str(int(x) + max_vals["community"] + 1) if x else x
            )
            nodes_df["title"] = nodes_df["title"].apply(lambda x: x + f"-{index_name}")
            nodes_df["source_id"] = nodes_df["source_id"].apply(
                lambda x: ",".join([i + f"-{index_name}" for i in x.split(",")])
            )
            max_vals["nodes"] = nodes_df["human_readable_id"].max()
            nodes_dfs.append(nodes_df)

            community_df = query_helper.get_df(community_report_table_path)
            for i in community_df["community"].astype(int):
                links["community"][i + max_vals["community"] + 1] = {
                    "index_name": sanitized_index_names_link[index_name],
                    "id": str(i),
                }
            if max_vals["community"] != -1:
                col = community_df["community"].astype(int) + max_vals["community"] + 1
                community_df["community"] = col.astype(str)
            max_vals["community"] = community_df["community"].astype(int).max()
            community_dfs.append(community_df)

            entities_df = query_helper.get_df(entities_table_path)
            for i in entities_df["human_readable_id"]:
                links["entities"][i + max_vals["entities"] + 1] = {
                    "index_name": sanitized_index_names_link[index_name],
                    "id": i,
                }
            if max_vals["entities"] != -1:
                entities_df["human_readable_id"] += max_vals["entities"] + 1
            entities_df["name"] = entities_df["name"].apply(
                lambda x: x + f"-{index_name}"
            )
            entities_df["text_unit_ids"] = entities_df["text_unit_ids"].apply(
                lambda x: [i + f"-{index_name}" for i in x]
            )
            max_vals["entities"] = entities_df["human_readable_id"].max()
            entities_dfs.append(entities_df)

        # merge the dataframes
        nodes_combined = pd.concat(nodes_dfs, axis=0, ignore_index=True, sort=False)
        community_combined = pd.concat(
            community_dfs, axis=0, ignore_index=True, sort=False
        )
        entities_combined = pd.concat(
            entities_dfs, axis=0, ignore_index=True, sort=False
        )

        # load custom pipeline settings
        this_directory = os.path.dirname(
            os.path.abspath(inspect.getfile(inspect.currentframe()))
        )
        data = yaml.safe_load(open(f"{this_directory}/pipeline-settings.yaml"))
        # layer the custom settings on top of the default configuration settings of graphrag
        parameters = create_graphrag_config(data, ".")

        # perform async search
        result = await global_search(
            config=parameters,
            nodes=nodes_combined,
            entities=entities_combined,
            community_reports=community_combined,
            community_level=COMMUNITY_LEVEL,
            response_type="Multiple Paragraphs",
            query=request.query,
        )

        # link index provenance to the context data
        context_data = _update_context(result[1], links)

        return GraphResponse(result=result[0], context_data=context_data)
    except Exception as e:
        reporter = ReporterSingleton().get_instance()
        reporter.on_error(
            message="Could not perform global search.",
            cause=e,
            stack=traceback.format_exc(),
        )
        raise HTTPException(status_code=500, detail=None)


@query_route.post(
    "/local",
    summary="Perform a local search across the knowledge graph index.",
    description="The local query method generates answers by combining relevant data from the AI-extracted knowledge-graph with text chunks of the raw documents. This method is suitable for questions that require an understanding of specific entities mentioned in the documents (e.g. What are the healing properties of chamomile?).",
    response_model=GraphResponse,
    responses={200: {"model": GraphResponse}},
)
async def local_query(request: GraphRequest):
    if isinstance(request.index_name, str):
        index_names = [request.index_name]
    else:
        index_names = request.index_name
    sanitized_index_names = [sanitize_name(name) for name in index_names]
    sanitized_index_names_link = {
        s: i for s, i in zip(sanitized_index_names, index_names)
    }

    for index_name in sanitized_index_names:
        if not _is_index_complete(index_name):
            raise HTTPException(
                status_code=500,
                detail=f"{index_name} not ready for querying.",
            )

    blob_service_client = BlobServiceClientSingleton.get_instance()

    community_dfs = []
    covariates_dfs = []
    entities_dfs = []
    nodes_dfs = []
    relationships_dfs = []
    text_units_dfs = []

    links = {
        "nodes": {},
        "community": {},
        "entities": {},
        "text_units": {},
        "relationships": {},
        "covariates": {},
    }
    max_vals = {
        "nodes": -1,
        "community": -1,
        "entities": -1,
        "text_units": -1,
        "relationships": -1,
        "covariates": -1,
    }

    COMMUNITY_REPORT_TABLE = "output/create_final_community_reports.parquet"
    COVARIATES_TABLE = "output/create_final_covariates.parquet"
    ENTITIES_TABLE = "output/create_final_entities.parquet"
    NODES_TABLE = "output/create_final_nodes.parquet"
    RELATIONSHIPS_TABLE = "output/create_final_relationships.parquet"
    TEXT_UNITS_TABLE = "output/create_final_text_units.parquet"

    if isinstance(request.community_level, int):
        COMMUNITY_LEVEL = request.community_level
    else:
        # Current investigations show that community level 2 is the most useful for local search. Set this as the default value
        COMMUNITY_LEVEL = 2

    for index_name in sanitized_index_names:
        # check for existence of files the query relies on to validate the index is complete
        validate_index_file_exist(index_name, COMMUNITY_REPORT_TABLE)
        validate_index_file_exist(index_name, ENTITIES_TABLE)
        validate_index_file_exist(index_name, NODES_TABLE)
        validate_index_file_exist(index_name, RELATIONSHIPS_TABLE)
        validate_index_file_exist(index_name, TEXT_UNITS_TABLE)

        community_report_table_path = f"abfs://{index_name}/{COMMUNITY_REPORT_TABLE}"
        covariates_table_path = f"abfs://{index_name}/{COVARIATES_TABLE}"
        entities_table_path = f"abfs://{index_name}/{ENTITIES_TABLE}"
        nodes_table_path = f"abfs://{index_name}/{NODES_TABLE}"
        relationships_table_path = f"abfs://{index_name}/{RELATIONSHIPS_TABLE}"
        text_units_table_path = f"abfs://{index_name}/{TEXT_UNITS_TABLE}"

        # read the parquet files into DataFrames and add provenance information

        # note that nodes need to set before communities to that max community id makes sense
        nodes_df = query_helper.get_df(nodes_table_path)
        for i in nodes_df["human_readable_id"]:
            links["nodes"][i + max_vals["nodes"] + 1] = {
                "index_name": sanitized_index_names_link[index_name],
                "id": i,
            }
        if max_vals["nodes"] != -1:
            nodes_df["human_readable_id"] += max_vals["nodes"] + 1
        nodes_df["community"] = nodes_df["community"].apply(
            lambda x: str(int(x) + max_vals["community"] + 1) if x else x
        )
        nodes_df["id"] = nodes_df["id"].apply(lambda x: x + f"-{index_name}")
        nodes_df["title"] = nodes_df["title"].apply(lambda x: x + f"-{index_name}")
        nodes_df["source_id"] = nodes_df["source_id"].apply(
            lambda x: ",".join([i + f"-{index_name}" for i in x.split(",")])
        )
        max_vals["nodes"] = nodes_df["human_readable_id"].max()
        nodes_dfs.append(nodes_df)

        community_df = query_helper.get_df(community_report_table_path)
        for i in community_df["community"].astype(int):
            links["community"][i + max_vals["community"] + 1] = {
                "index_name": sanitized_index_names_link[index_name],
                "id": str(i),
            }
        if max_vals["community"] != -1:
            col = community_df["community"].astype(int) + max_vals["community"] + 1
            community_df["community"] = col.astype(str)
        max_vals["community"] = community_df["community"].astype(int).max()
        community_dfs.append(community_df)

        entities_df = query_helper.get_df(entities_table_path)
        for i in entities_df["human_readable_id"]:
            links["entities"][i + max_vals["entities"] + 1] = {
                "index_name": sanitized_index_names_link[index_name],
                "id": i,
            }
        if max_vals["entities"] != -1:
            entities_df["human_readable_id"] += max_vals["entities"] + 1
        entities_df["id"] = entities_df["id"].apply(lambda x: x + f"-{index_name}")
        entities_df["name"] = entities_df["name"].apply(lambda x: x + f"-{index_name}")
        entities_df["text_unit_ids"] = entities_df["text_unit_ids"].apply(
            lambda x: [i + f"-{index_name}" for i in x]
        )
        max_vals["entities"] = entities_df["human_readable_id"].max()
        entities_dfs.append(entities_df)

        relationships_df = query_helper.get_df(relationships_table_path)
        for i in relationships_df["human_readable_id"].astype(int):
            links["relationships"][i + max_vals["relationships"] + 1] = {
                "index_name": sanitized_index_names_link[index_name],
                "id": i,
            }
        if max_vals["relationships"] != -1:
            col = (
                relationships_df["human_readable_id"].astype(int)
                + max_vals["relationships"]
                + 1
            )
            relationships_df["human_readable_id"] = col.astype(str)
        relationships_df["source"] = relationships_df["source"].apply(
            lambda x: x + f"-{index_name}"
        )
        relationships_df["target"] = relationships_df["target"].apply(
            lambda x: x + f"-{index_name}"
        )
        relationships_df["text_unit_ids"] = relationships_df["text_unit_ids"].apply(
            lambda x: [i + f"-{index_name}" for i in x]
        )
        max_vals["relationships"] = (
            relationships_df["human_readable_id"].astype(int).max()
        )
        relationships_dfs.append(relationships_df)

        text_units_df = query_helper.get_df(text_units_table_path)
        text_units_df["id"] = text_units_df["id"].apply(lambda x: f"{x}-{index_name}")
        text_units_dfs.append(text_units_df)

        index_container_client = blob_service_client.get_container_client(index_name)
        if index_container_client.get_blob_client(COVARIATES_TABLE).exists():
            covariates_df = query_helper.get_df(covariates_table_path)
            if i in covariates_df["human_readable_id"].astype(int):
                links["covariates"][i + max_vals["covariates"] + 1] = {
                    "index_name": sanitized_index_names_link[index_name],
                    "id": i,
                }
            if max_vals["covariates"] != -1:
                col = (
                    covariates_df["human_readable_id"].astype(int)
                    + max_vals["covariates"]
                    + 1
                )
                covariates_df["human_readable_id"] = col.astype(str)
            max_vals["covariates"] = (
                covariates_df["human_readable_id"].astype(int).max()
            )
            covariates_dfs.append(covariates_df)

    nodes_combined = pd.concat(nodes_dfs, axis=0, ignore_index=True)
    community_combined = pd.concat(community_dfs, axis=0, ignore_index=True)
    entities_combined = pd.concat(entities_dfs, axis=0, ignore_index=True)
    text_units_combined = pd.concat(text_units_dfs, axis=0, ignore_index=True)
    relationships_combined = pd.concat(relationships_dfs, axis=0, ignore_index=True)
    covariates_combined = (
        pd.concat(covariates_dfs, axis=0, ignore_index=True)
        if covariates_dfs != []
        else None
    )

    # load custom pipeline settings
    this_directory = os.path.dirname(
        os.path.abspath(inspect.getfile(inspect.currentframe()))
    )
    data = yaml.safe_load(open(f"{this_directory}/pipeline-settings.yaml"))
    # layer the custom settings on top of the default configuration settings of graphrag
    parameters = create_graphrag_config(data, ".")

    # add index_names to vector_store args
    parameters.embeddings.vector_store["index_names"] = sanitized_index_names
    # internally write over the get_embedding_description_store
    # method to use the multi-index collection.
    import graphrag.query.api

    graphrag.query.api._get_embedding_description_store = (
        _get_embedding_description_store
    )
    # perform async search
    result = await local_search(
        config=parameters,
        nodes=nodes_combined,
        entities=entities_combined,
        community_reports=community_combined,
        text_units=text_units_combined,
        relationships=relationships_combined,
        covariates=covariates_combined,
        community_level=COMMUNITY_LEVEL,
        response_type="Multiple Paragraphs",
        query=request.query,
    )

    # link index provenance to the context data
    context_data = _update_context(result[1], links)

    return GraphResponse(result=result[0], context_data=context_data)


def _is_index_complete(index_name: str) -> bool:
    """
    Check if an index is ready for querying.

    An index is ready for use only if it exists in the jobs table in cosmos db and
    the indexing build job has finished (i.e. 100 percent). Otherwise it is not ready.

    Args:
    -----
    index_name (str)
        Name of the index to check.

    Returns: bool
        True if the index is ready for use, False otherwise.
    """
    if PipelineJob.item_exist(index_name):
        pipeline_job = PipelineJob.load_item(index_name)
        if PipelineJobState(pipeline_job.status) == PipelineJobState.COMPLETE:
            return True
    return False


def _update_context(context, links):
    """
    Update context data.
    context_keys = ['reports', 'entities', 'relationships', 'claims', 'sources']
    """
    updated_context = {}
    for key in context:
        updated_entry = []
        if key == "reports":
            updated_entry = [
                dict(
                    {k: entry[k] for k in entry},
                    **{
                        "index_name": links["community"][int(entry["id"])][
                            "index_name"
                        ],
                        "index_id": links["community"][int(entry["id"])]["id"],
                    },
                )
                for entry in context[key]
            ]
        if key == "entities":
            updated_entry = [
                dict(
                    {k: entry[k] for k in entry},
                    **{
                        "entity": entry["entity"].split("-")[0],
                        "index_name": links["entities"][int(entry["id"])]["index_name"],
                        "index_id": links["entities"][int(entry["id"])]["id"],
                    },
                )
                for entry in context[key]
            ]
        if key == "relationships":
            updated_entry = [
                dict(
                    {k: entry[k] for k in entry},
                    **{
                        "source": entry["source"].split("-")[0],
                        "target": entry["target"].split("-")[0],
                        "index_name": links["relationships"][int(entry["id"])][
                            "index_name"
                        ],
                        "index_id": links["relationships"][int(entry["id"])]["id"],
                    },
                )
                for entry in context[key]
            ]
        if key == "claims":
            updated_entry = [
                dict(
                    {k: entry[k] for k in entry},
                    **{
                        "index_name": links["claims"][int(entry["id"])]["index_name"],
                        "index_id": links["claims"][int(entry["id"])]["id"],
                    },
                )
                for entry in context[key]
            ]
        if key == "sources":
            updated_entry = context[key]
        updated_context[key] = updated_entry
    return updated_context


def _get_embedding_description_store(
    entities: Any,
    vector_store_type: str = Any,
    config_args: dict | None = None,
):
    collection_names = [
        f"{index_name}_description_embedding"
        for index_name in config_args.get("index_names", [])
    ]
    ai_search_url = os.environ["AI_SEARCH_URL"]
    description_embedding_store = MultiAzureAISearch(
        collection_name="multi",
        document_collection=None,
        db_connection=None,
    )
    description_embedding_store.connect(url=ai_search_url)
    for collection_name in collection_names:
        description_embedding_store.add_collection(collection_name)
    return description_embedding_store


class MultiAzureAISearch(BaseVectorStore):
    """The Azure AI Search vector storage implementation."""

    def __init__(
        self,
        collection_name: str,
        db_connection: Any,
        document_collection: Any,
        query_filter: Any | None = None,
        **kwargs: Any,
    ):
        self.collection_name = collection_name
        self.db_connection = db_connection
        self.document_collection = document_collection
        self.query_filter = query_filter
        self.kwargs = kwargs
        self.collections = []

    def add_collection(self, collection_name: str):
        self.collections.append(collection_name)

    def connect(self, **kwargs: Any) -> Any:
        """Connect to the AzureAI vector store."""
        self.url = kwargs.get("url", None)
        self.vector_size = kwargs.get("vector_size", 1536)

        self.vector_search_profile_name = kwargs.get(
            "vector_search_profile_name", "vectorSearchProfile"
        )

        if self.url:
            pass
        else:
            not_supported_error = (
                "Azure AI Search client is not supported on local host."
            )
            raise ValueError(not_supported_error)

    def load_documents(
        self, documents: list[VectorStoreDocument], overwrite: bool = True
    ) -> None:
        raise NotImplementedError("load_documents() method not implemented")

    def filter_by_id(self, include_ids: list[str] | list[int]) -> Any:
        """Build a query filter to filter documents by a list of ids."""
        if include_ids is None or len(include_ids) == 0:
            self.query_filter = None
            # returning to keep consistency with other methods, but not needed
            return self.query_filter

        # more info about odata filtering here: https://learn.microsoft.com/en-us/azure/search/search-query-odata-search-in-function
        # search.in is faster that joined and/or conditions
        id_filter = ",".join([f"{id!s}" for id in include_ids])
        self.query_filter = f"search.in(id, '{id_filter}', ',')"

        # returning to keep consistency with other methods, but not needed
        # TODO: Refactor on a future PR
        return self.query_filter

    def similarity_search_by_vector(
        self, query_embedding: list[float], k: int = 10, **kwargs: Any
    ) -> list[VectorStoreSearchResult]:
        """Perform a vector-based similarity search."""
        vectorized_query = VectorizedQuery(
            vector=query_embedding, k_nearest_neighbors=k, fields="vector"
        )

        docs = []
        for collection_name in self.collections:
            add_on = "-" + str(collection_name.split("_")[0])
            audience = os.environ["AI_SEARCH_AUDIENCE"]
            db_connection = SearchClient(
                self.url,
                collection_name,
                DefaultAzureCredential(),
                audience=audience,
            )
            response = db_connection.search(
                vector_queries=[vectorized_query],
            )
            mod_response = []
            for r in response:
                r["id"] = r.get("id", "") + add_on
                mod_response += [r]
            docs += mod_response
        return [
            VectorStoreSearchResult(
                document=VectorStoreDocument(
                    id=doc.get("id", ""),
                    text=doc.get("text", ""),
                    vector=doc.get("vector", []),
                    attributes=(json.loads(doc.get("attributes", "{}"))),
                ),
                score=abs(doc["@search.score"]),
            )
            for doc in docs
        ]

    def similarity_search_by_text(
        self, text: str, text_embedder: TextEmbedder, k: int = 10, **kwargs: Any
    ) -> list[VectorStoreSearchResult]:
        """Perform a text-based similarity search."""
        query_embedding = text_embedder(text)
        if query_embedding:
            return self.similarity_search_by_vector(
                query_embedding=query_embedding, k=k
            )
        return []

================
File: backend/src/api/source.py
================
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT License.

import os

import pandas as pd
from azure.identity import DefaultAzureCredential
from fastapi import APIRouter, HTTPException

from src.api.common import (
    sanitize_name,
    validate_index_file_exist,
)
from src.models import (
    ClaimResponse,
    EntityResponse,
    RelationshipResponse,
    ReportResponse,
    TextUnitResponse,
)
from src.reporting import ReporterSingleton

source_route = APIRouter(
    prefix="/source",
    tags=["Sources"],
)


COMMUNITY_REPORT_TABLE = "output/create_final_community_reports.parquet"
COVARIATES_TABLE = "output/create_final_covariates.parquet"
ENTITY_EMBEDDING_TABLE = "output/create_final_entities.parquet"
RELATIONSHIPS_TABLE = "output/create_final_relationships.parquet"
TEXT_UNITS_TABLE = "output/create_base_text_units.parquet"
DOCUMENTS_TABLE = "output/create_base_documents.parquet"
storage_account_blob_url = os.environ["STORAGE_ACCOUNT_BLOB_URL"]
storage_account_name = storage_account_blob_url.split("//")[1].split(".")[0]
storage_account_host = storage_account_blob_url.split("//")[1]
storage_options = {
    "account_name": storage_account_name,
    "account_host": storage_account_host,
    "credential": DefaultAzureCredential(),
}


@source_route.get(
    "/report/{index_name}/{report_id}",
    summary="Return a single community report.",
    response_model=ReportResponse,
    responses={200: {"model": ReportResponse}},
)
async def get_report_info(index_name: str, report_id: str):
    # check for existence of file the query relies on to validate the index is complete
    sanitized_index_name = sanitize_name(index_name)
    validate_index_file_exist(sanitized_index_name, COMMUNITY_REPORT_TABLE)
    try:
        report_table = pd.read_parquet(
            f"abfs://{sanitized_index_name}/{COMMUNITY_REPORT_TABLE}",
            storage_options=storage_options,
        )
        row = report_table[report_table.community == report_id]
        return ReportResponse(text=row["full_content"].values[0])
    except Exception:
        reporter = ReporterSingleton().get_instance()
        reporter.on_error("Could not get report.")
        raise HTTPException(
            status_code=500,
            detail=f"Error retrieving report '{report_id}' from index '{index_name}'.",
        )


@source_route.get(
    "/text/{index_name}/{text_unit_id}",
    summary="Return a single base text unit.",
    response_model=TextUnitResponse,
    responses={200: {"model": TextUnitResponse}},
)
async def get_chunk_info(index_name: str, text_unit_id: str):
    # check for existence of file the query relies on to validate the index is complete
    sanitized_index_name = sanitize_name(index_name)
    validate_index_file_exist(sanitized_index_name, TEXT_UNITS_TABLE)
    validate_index_file_exist(sanitized_index_name, DOCUMENTS_TABLE)
    try:
        text_unit_table = pd.read_parquet(
            f"abfs://{sanitized_index_name}/{TEXT_UNITS_TABLE}",
            storage_options=storage_options,
        )
        docs = pd.read_parquet(
            f"abfs://{sanitized_index_name}/{DOCUMENTS_TABLE}",
            storage_options=storage_options,
        )
        links = {
            el["id"]: el["title"]
            for el in docs[["id", "title"]].to_dict(orient="records")
        }
        text_unit_table["source_doc"] = text_unit_table["document_ids"].apply(
            lambda x: links[x[0]]
        )
        row = text_unit_table[text_unit_table.chunk_id == text_unit_id][
            ["chunk", "source_doc"]
        ]
        return TextUnitResponse(
            text=row["chunk"].values[0], source_document=row["source_doc"].values[0]
        )
    except Exception:
        reporter = ReporterSingleton().get_instance()
        reporter.on_error("Could not get text chunk.")
        raise HTTPException(
            status_code=500,
            detail=f"Error retrieving text chunk '{text_unit_id}' from index '{index_name}'.",
        )


@source_route.get(
    "/entity/{index_name}/{entity_id}",
    summary="Return a single entity.",
    response_model=EntityResponse,
    responses={200: {"model": EntityResponse}},
)
async def get_entity_info(index_name: str, entity_id: int):
    # check for existence of file the query relies on to validate the index is complete
    sanitized_index_name = sanitize_name(index_name)
    validate_index_file_exist(sanitized_index_name, ENTITY_EMBEDDING_TABLE)
    try:
        entity_table = pd.read_parquet(
            f"abfs://{sanitized_index_name}/{ENTITY_EMBEDDING_TABLE}",
            storage_options=storage_options,
        )
        row = entity_table[entity_table.human_readable_id == entity_id]
        return EntityResponse(
            name=row["name"].values[0],
            description=row["description"].values[0],
            text_units=row["text_unit_ids"].values[0].tolist(),
        )
    except Exception:
        reporter = ReporterSingleton().get_instance()
        reporter.on_error("Could not get entity")
        raise HTTPException(
            status_code=500,
            detail=f"Error retrieving entity '{entity_id}' from index '{index_name}'.",
        )


@source_route.get(
    "/claim/{index_name}/{claim_id}",
    summary="Return a single claim.",
    response_model=ClaimResponse,
    responses={200: {"model": ClaimResponse}},
)
async def get_claim_info(index_name: str, claim_id: int):
    # check for existence of file the query relies on to validate the index is complete
    # claims is optional in graphrag
    sanitized_index_name = sanitize_name(index_name)
    try:
        validate_index_file_exist(sanitized_index_name, COVARIATES_TABLE)
    except ValueError:
        raise HTTPException(
            status_code=500,
            detail=f"Claim data unavailable for index '{index_name}'.",
        )
    try:
        claims_table = pd.read_parquet(
            f"abfs://{sanitized_index_name}/{COVARIATES_TABLE}",
            storage_options=storage_options,
        )
        claims_table.human_readable_id = claims_table.human_readable_id.astype(
            float
        ).astype(int)
        row = claims_table[claims_table.human_readable_id == claim_id]
        return ClaimResponse(
            covariate_type=row["covariate_type"].values[0],
            type=row["type"].values[0],
            description=row["description"].values[0],
            subject_id=row["subject_id"].values[0],
            object_id=row["object_id"].values[0],
            source_text=row["source_text"].values[0],
            text_unit_id=row["text_unit_id"].values[0],
            document_ids=row["document_ids"].values[0].tolist(),
        )
    except Exception:
        reporter = ReporterSingleton().get_instance()
        reporter.on_error("Could not get claim.")
        raise HTTPException(
            status_code=500,
            detail=f"Error retrieving claim '{claim_id}' from index '{index_name}'.",
        )


@source_route.get(
    "/relationship/{index_name}/{relationship_id}",
    summary="Return a single relationship.",
    response_model=RelationshipResponse,
    responses={200: {"model": RelationshipResponse}},
)
async def get_relationship_info(index_name: str, relationship_id: int):
    # check for existence of file the query relies on to validate the index is complete
    sanitized_index_name = sanitize_name(index_name)
    validate_index_file_exist(sanitized_index_name, RELATIONSHIPS_TABLE)
    validate_index_file_exist(sanitized_index_name, ENTITY_EMBEDDING_TABLE)
    try:
        relationship_table = pd.read_parquet(
            f"abfs://{sanitized_index_name}/{RELATIONSHIPS_TABLE}",
            storage_options=storage_options,
        )
        entity_table = pd.read_parquet(
            f"abfs://{sanitized_index_name}/{ENTITY_EMBEDDING_TABLE}",
            storage_options=storage_options,
        )
        row = relationship_table[
            relationship_table.human_readable_id == str(relationship_id)
        ]
        return RelationshipResponse(
            source=row["source"].values[0],
            source_id=entity_table[
                entity_table.name == row["source"].values[0]
            ].human_readable_id.values[0],
            target=row["target"].values[0],
            target_id=entity_table[
                entity_table.name == row["target"].values[0]
            ].human_readable_id.values[0],
            description=row["description"].values[0],
            text_units=[
                x[0] for x in row["text_unit_ids"].to_list()
            ],  # extract text_unit_ids from a list of panda series
        )
    except Exception:
        reporter = ReporterSingleton().get_instance()
        reporter.on_error("Could not get relationship.")
        raise HTTPException(
            status_code=500,
            detail=f"Error retrieving relationship '{relationship_id}' from index '{index_name}'.",
        )

================
File: backend/src/reporting/__init__.py
================
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT License.

from src.reporting.application_insights_workflow_callbacks import (
    ApplicationInsightsWorkflowCallbacks,
)
from src.reporting.console_workflow_callbacks import ConsoleWorkflowCallbacks
from src.reporting.load_reporter import load_pipeline_reporter
from src.reporting.reporter_singleton import ReporterSingleton
from src.reporting.typing import (
    PipelineAppInsightsReportingConfig,
    PipelineReportingConfigTypes,
    Reporters,
)

__all__ = [
    "Reporters",
    "ApplicationInsightsWorkflowCallbacks",
    "ConsoleWorkflowCallbacks",
    "ReporterSingleton",
    "PipelineAppInsightsReportingConfig",
    "PipelineReportingConfigTypes",
    "load_pipeline_reporter",
]

================
File: backend/src/reporting/application_insights_workflow_callbacks.py
================
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT License.

import hashlib
import logging
import time

# from dataclasses import asdict
from typing import (
    Any,
    Dict,
    Optional,
)

from azure.monitor.opentelemetry.exporter import AzureMonitorLogExporter
from datashaper.workflow.workflow_callbacks import NoopWorkflowCallbacks
from opentelemetry._logs import (
    get_logger_provider,
    set_logger_provider,
)
from opentelemetry.sdk._logs import (
    LoggerProvider,
    LoggingHandler,
)
from opentelemetry.sdk._logs.export import BatchLogRecordProcessor


class ApplicationInsightsWorkflowCallbacks(NoopWorkflowCallbacks):
    """A reporter that writes to an AppInsights Workspace."""

    _logger: logging.Logger
    _logger_name: str
    _logger_level: int
    _logger_level_name: str
    _properties: Dict[str, Any]
    _workflow_name: str
    _index_name: str
    _num_workflow_steps: int
    _processed_workflow_steps: list[str] = []

    def __init__(
        self,
        connection_string: str,
        logger_name: str | None = None,
        logger_level: int = logging.INFO,
        index_name: str = "",
        num_workflow_steps: int = 0,
        properties: Dict[str, Any] = {},
    ):
        """
        Initialize the AppInsightsReporter.

        Args:
            connection_string (str): The connection string for the App Insights instance.
            logger_name (str | None, optional): The name of the logger. Defaults to None.
            logger_level (int, optional): The logging level. Defaults to logging.INFO.
            index_name (str, optional): The name of an index. Defaults to "".
            num_workflow_steps (int): A list of workflow names ordered by their execution. Defaults to [].
            properties (Dict[str, Any], optional): Additional properties to be included in the log. Defaults to {}.
        """
        self._logger: logging.Logger
        self._logger_name = logger_name
        self._logger_level = logger_level
        self._logger_level_name: str = logging.getLevelName(logger_level)
        self._properties = properties
        self._workflow_name = "N/A"
        self._index_name = index_name
        self._num_workflow_steps = num_workflow_steps
        self._processed_workflow_steps = []  # maintain a running list of workflow steps that get processed
        """Create a new logger with an AppInsights handler."""
        self.__init_logger(connection_string=connection_string)

    def __init_logger(self, connection_string, max_logger_init_retries: int = 10):
        max_retry = max_logger_init_retries
        while not (hasattr(self, "_logger")):
            if max_retry == 0:
                raise Exception(
                    "Failed to create logger. Could not disambiguate logger name."
                )

            # generate a unique logger name
            current_time = str(time.time())
            unique_hash = hashlib.sha256(current_time.encode()).hexdigest()
            self._logger_name = f"{self.__class__.__name__}-{unique_hash}"
            if self._logger_name not in logging.Logger.manager.loggerDict:
                # attach azure monitor log exporter to logger provider
                logger_provider = LoggerProvider()
                set_logger_provider(logger_provider)
                exporter = AzureMonitorLogExporter(connection_string=connection_string)
                get_logger_provider().add_log_record_processor(
                    BatchLogRecordProcessor(
                        exporter=exporter,
                        schedule_delay_millis=60000,
                    )
                )
                # instantiate new logger
                self._logger = logging.getLogger(self._logger_name)
                self._logger.propagate = False
                # remove any existing handlers
                self._logger.handlers.clear()
                # fetch handler from logger provider and attach to class
                self._logger.addHandler(LoggingHandler())
                # set logging level
                self._logger.setLevel(logging.DEBUG)

            # reduce sentinel counter value
            max_retry -= 1

    def _format_details(self, details: Dict[str, Any] | None = None) -> Dict[str, Any]:
        """
        Format the details dictionary to comply with the Application Insights structured
        logging Property column standard.

        Args:
            details (Dict[str, Any] | None): Optional dictionary containing additional details to log.

        Returns:
            Dict[str, Any]: The formatted details dictionary with custom dimensions.
        """
        if not isinstance(details, dict) or (details is None):
            details = {}
        return {"custom_dimensions": {**self._properties, **unwrap_dict(details)}}

    def on_workflow_start(self, name: str, instance: object) -> None:
        """Execute this callback when a workflow starts."""
        self._workflow_name = name
        self._processed_workflow_steps.append(name)
        message = f"Index: {self._index_name} -- " if self._index_name else ""
        workflow_progress = (
            f" ({len(self._processed_workflow_steps)}/{self._num_workflow_steps})"
            if self._num_workflow_steps
            else ""
        )  # will take the form "(1/4)"
        message += f"Workflow{workflow_progress}: {name} started."
        details = {
            "workflow_name": name,
            # "workflow_instance": str(instance),
        }
        if self._index_name:
            details["index_name"] = self._index_name
        self._logger.info(
            message, stack_info=False, extra=self._format_details(details=details)
        )

    def on_workflow_end(self, name: str, instance: object) -> None:
        """Execute this callback when a workflow ends."""
        message = f"Index: {self._index_name} -- " if self._index_name else ""
        workflow_progress = (
            f" ({len(self._processed_workflow_steps)}/{self._num_workflow_steps})"
            if self._num_workflow_steps
            else ""
        )  # will take the form "(1/4)"
        message += f"Workflow{workflow_progress}: {name} complete."
        details = {
            "workflow_name": name,
            # "workflow_instance": str(instance),
        }
        if self._index_name:
            details["index_name"] = self._index_name
        self._logger.info(
            message, stack_info=False, extra=self._format_details(details=details)
        )

    def on_error(
        self,
        message: str,
        cause: Optional[BaseException] = None,
        stack: Optional[str] = None,
        details: Optional[dict] = None,
    ) -> None:
        """A call back handler for when an error occurs."""
        details = {} if details is None else details
        details = {"cause": str(cause), "stack": stack, **details}
        self._logger.error(
            message,
            exc_info=True,
            stack_info=False,
            extra=self._format_details(details=details),
        )

    def on_warning(self, message: str, details: Optional[dict] = None) -> None:
        """A call back handler for when a warning occurs."""
        self._logger.warning(
            message, stack_info=False, extra=self._format_details(details=details)
        )

    def on_log(self, message: str, details: Optional[dict] = None) -> None:
        """A call back handler for when a log message occurs."""
        self._logger.info(
            message, stack_info=False, extra=self._format_details(details=details)
        )

    def on_measure(
        self, name: str, value: float, details: Optional[dict] = None
    ) -> None:
        """A call back handler for when a measurement occurs."""
        pass


def unwrap_dict(input_dict, parent_key="", sep="_"):
    """
    Recursively unwraps a nested dictionary by flattening it into a single-level dictionary.

    Args:
        input_dict (dict): The input dictionary to be unwrapped.
        parent_key (str, optional): The parent key to be prepended to the keys of the unwrapped dictionary. Defaults to ''.
        sep (str, optional): The separator to be used between the parent key and the child key. Defaults to '_'.

    Returns:
        dict: The unwrapped dictionary.
    """
    items = []
    for k, v in input_dict.items():
        new_key = f"{parent_key}{sep}{k}" if parent_key else k
        if isinstance(v, dict):
            items.extend(unwrap_dict(v, new_key, sep=sep).items())
        else:
            items.append((new_key, v))
    return dict(items)

================
File: backend/src/reporting/blob_workflow_callbacks.py
================
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT License.

# from dataclasses import asdict
from datetime import datetime
from typing import (
    Any,
    Optional,
)

from azure.identity import DefaultAzureCredential
from azure.storage.blob import BlobServiceClient
from datashaper import NoopWorkflowCallbacks
from devtools import pformat


class BlobWorkflowCallbacks(NoopWorkflowCallbacks):
    """A reporter that writes to a blob storage."""

    _blob_service_client: BlobServiceClient
    _container_name: str
    _index_name: str
    _num_workflow_steps: int
    _processed_workflow_steps: list[str] = []
    _max_block_count: int = 25000  # 25k blocks per blob

    def __init__(
        self,
        storage_account_blob_url: str,
        container_name: str,
        blob_name: str = "",
        index_name: str = "",
        num_workflow_steps: int = 0,
    ):
        """Create a new instance of the BlobStorageReporter class.

        Args:
            storage_account_blob_url (str): The URL to the storage account.
            container_name (str): The name of the container.
            blob_name (str, optional): The name of the blob. Defaults to "".
            index_name (str, optional): The name of the index. Defaults to "".
            num_workflow_steps (int): A list of workflow names ordered by their execution. Defaults to [].
        """
        self._storage_account_blob_url = storage_account_blob_url
        credential = DefaultAzureCredential()
        self._blob_service_client = BlobServiceClient(
            storage_account_blob_url, credential=credential
        )
        if not blob_name:
            blob_name = f"{container_name}/{datetime.now().strftime('%Y-%m-%d-%H:%M:%S:%f')}.logs.txt"
        self._index_name = index_name
        self._num_workflow_steps = num_workflow_steps
        self._processed_workflow_steps = []  # maintain a running list of workflow steps that get processed
        self._blob_name = blob_name
        self._container_name = container_name
        self._blob_client = self._blob_service_client.get_blob_client(
            self._container_name, self._blob_name
        )
        if not self._blob_client.exists():
            self._blob_client.create_append_blob()
        self._num_blocks = 0  # refresh block counter

    def _write_log(self, log: dict[str, Any]):
        # create a new file when block count hits close 25k
        if self._num_blocks >= self._max_block_count:
            self.__init__(self._storage_account_blob_url, self._container_name)
        blob_client = self._blob_service_client.get_blob_client(
            self._container_name, self._blob_name
        )
        blob_client.append_block(pformat(log, indent=2) + "\n")
        self._num_blocks += 1

    def on_error(
        self,
        message: str,
        cause: BaseException | None = None,
        stack: str | None = None,
        details: dict | None = None,
    ):
        """Report an error."""
        self._write_log({
            "type": "error",
            "data": message,
            "cause": str(cause),
            "stack": stack,
            "details": details,
        })

    def on_workflow_start(self, name: str, instance: object) -> None:
        """Execute this callback when a workflow starts."""
        self._workflow_name = name
        self._processed_workflow_steps.append(name)
        message = f"Index: {self._index_name} -- " if self._index_name else ""
        workflow_progress = (
            f" ({len(self._processed_workflow_steps)}/{self._num_workflow_steps})"
            if self._num_workflow_steps
            else ""
        )  # will take the form "(1/4)"
        message += f"Workflow{workflow_progress}: {name} started."
        details = {
            "workflow_name": name,
            # "workflow_instance": str(instance),
        }
        if self._index_name:
            details["index_name"] = self._index_name
        self._write_log({
            "type": "on_workflow_start",
            "data": message,
            "details": details,
        })

    def on_workflow_end(self, name: str, instance: object) -> None:
        """Execute this callback when a workflow ends."""
        message = f"Index: {self._index_name} -- " if self._index_name else ""
        workflow_progress = (
            f" ({len(self._processed_workflow_steps)}/{self._num_workflow_steps})"
            if self._num_workflow_steps
            else ""
        )  # will take the form "(1/4)"
        message += f"Workflow{workflow_progress}: {name} complete."
        details = {
            "workflow_name": name,
            # "workflow_instance": str(instance),
        }
        if self._index_name:
            details["index_name"] = self._index_name
        self._write_log({
            "type": "on_workflow_end",
            "data": message,
            "details": details,
        })

    def on_warning(self, message: str, details: dict | None = None):
        """Report a warning."""
        self._write_log({"type": "warning", "data": message, "details": details})

    def on_log(self, message: str, details: dict | None = None):
        """Report a generic log message."""
        self._write_log({"type": "log", "data": message, "details": details})

    def on_measure(
        self, name: str, value: float, details: Optional[dict] = None
    ) -> None:
        """A call back handler for when a measurement occurs."""
        pass

================
File: backend/src/reporting/console_workflow_callbacks.py
================
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT License.

import hashlib
import logging
import sys
import time
from typing import (
    Any,
    Dict,
    Optional,
)

from datashaper.workflow.workflow_callbacks import NoopWorkflowCallbacks


class ConsoleWorkflowCallbacks(NoopWorkflowCallbacks):
    """A reporter that writes to a stream (sys.stdout)."""

    _logger: logging.Logger
    _logger_name: str
    _logger_level: int
    _logger_level_name: str
    _properties: Dict[str, Any]
    _workflow_name: str
    _index_name: str
    _num_workflow_steps: int
    _processed_workflow_steps: list[str] = []

    def __init__(
        self,
        logger_name: str | None = None,
        logger_level: int = logging.INFO,
        index_name: str = "",
        num_workflow_steps: int = 0,
        properties: Dict[str, Any] = {},
    ):
        """
        Initialize the ConsoleWorkflowCallbacks.

        Args:
            logger_name (str | None, optional): The name of the logger. Defaults to None.
            logger_level (int, optional): The logging level. Defaults to logging.INFO.
            index_name (str, optional): The name of an index. Defaults to "".
            num_workflow_steps (int): A list of workflow names ordered by their execution. Defaults to [].
            properties (Dict[str, Any], optional): Additional properties to be included in the log. Defaults to {}.
        """
        self._logger: logging.Logger
        self._logger_name = logger_name
        self._logger_level = logger_level
        self._logger_level_name: str = logging.getLevelName(logger_level)
        self._properties = properties
        self._workflow_name = "N/A"
        self._index_name = index_name
        self._num_workflow_steps = num_workflow_steps
        self._processed_workflow_steps = []  # maintain a running list of workflow steps that get processed
        """Create a new logger with an AppInsights handler."""
        self.__init_logger()

    def __init_logger(self, max_logger_init_retries: int = 10):
        max_retry = max_logger_init_retries
        while not (hasattr(self, "_logger")):
            if max_retry == 0:
                raise Exception(
                    "Failed to create logger. Could not disambiguate logger name."
                )

            # generate a unique logger name
            current_time = str(time.time())
            unique_hash = hashlib.sha256(current_time.encode()).hexdigest()
            self._logger_name = f"{self.__class__.__name__}-{unique_hash}"
            if self._logger_name not in logging.Logger.manager.loggerDict:
                # instantiate new logger
                self._logger = logging.getLogger(self._logger_name)
                self._logger.propagate = False
                # remove any existing handlers
                self._logger.handlers.clear()
                # create a console handler
                handler = logging.StreamHandler(stream=sys.stdout)
                # create a formatter and include 'extra_details' in the format string
                handler.setFormatter(
                    # logging.Formatter(
                    #     "[%(levelname)s] %(asctime)s - %(message)s \n %(stack)s"
                    # )
                    logging.Formatter("[%(levelname)s] %(asctime)s - %(message)s")
                )
                self._logger.addHandler(handler)
                # set logging level
                self._logger.setLevel(logging.INFO)

            # reduce sentinel counter value
            max_retry -= 1

    def _format_details(self, details: Dict[str, Any] | None = None) -> Dict[str, Any]:
        """
        Format the details dictionary to comply with the Application Insights structured.

        logging Property column standard.

        Args:
            details (Dict[str, Any] | None): Optional dictionary containing additional details to log.

        Returns:
            Dict[str, Any]: The formatted details dictionary with custom dimensions.
        """
        if not isinstance(details, dict) or (details is None):
            details = {}
        return {**self._properties, **details}

    def on_workflow_start(self, name: str, instance: object) -> None:
        """Execute this callback when a workflow starts."""
        self._workflow_name = name
        self._processed_workflow_steps.append(name)
        message = f"Index: {self._index_name} -- " if self._index_name else ""
        workflow_progress = (
            f" ({len(self._processed_workflow_steps)}/{self._num_workflow_steps})"
            if self._num_workflow_steps
            else ""
        )  # will take the form "(1/4)"
        message += f"Workflow{workflow_progress}: {name} started."
        details = {
            "workflow_name": name,
            # "workflow_instance": str(instance),
        }
        if self._index_name:
            details["index_name"] = self._index_name
        self._logger.info(
            message, stack_info=False, extra=self._format_details(details=details)
        )

    def on_workflow_end(self, name: str, instance: object) -> None:
        """Execute this callback when a workflow ends."""
        message = f"Index: {self._index_name} -- " if self._index_name else ""
        workflow_progress = (
            f" ({len(self._processed_workflow_steps)}/{self._num_workflow_steps})"
            if self._num_workflow_steps
            else ""
        )  # will take the form "(1/4)"
        message += f"Workflow{workflow_progress}: {name} complete."
        details = {
            "workflow_name": name,
            # "workflow_instance": str(instance),
        }
        if self._index_name:
            details["index_name"] = self._index_name
        self._logger.info(
            message, stack_info=False, extra=self._format_details(details=details)
        )

    def on_error(
        self,
        message: str,
        cause: Optional[BaseException] = None,
        stack: Optional[str] = None,
        details: Optional[dict] = None,
    ) -> None:
        """A call back handler for when an error occurs."""
        details = {} if details is None else details
        details = {"cause": str(cause), "stack": stack, **details}
        self._logger.error(
            message,
            exc_info=True,
            stack_info=False,
            extra=self._format_details(details=details),
        )

    def on_warning(self, message: str, details: Optional[dict] = None) -> None:
        """A call back handler for when a warning occurs."""
        self._logger.warning(
            message, stack_info=False, extra=self._format_details(details=details)
        )

    def on_log(self, message: str, details: Optional[dict] = None) -> None:
        """A call back handler for when a log message occurs."""
        self._logger.info(
            message, stack_info=False, extra=self._format_details(details=details)
        )

    def on_measure(
        self, name: str, value: float, details: Optional[dict] = None
    ) -> None:
        """A call back handler for when a measurement occurs."""
        pass

================
File: backend/src/reporting/load_reporter.py
================
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT License.

import os
from datetime import datetime
from pathlib import Path
from typing import List

from datashaper import WorkflowCallbacks, WorkflowCallbacksManager
from graphrag.index.reporting import FileWorkflowCallbacks

from src.api.azure_clients import BlobServiceClientSingleton
from src.reporting.application_insights_workflow_callbacks import (
    ApplicationInsightsWorkflowCallbacks,
)
from src.reporting.blob_workflow_callbacks import BlobWorkflowCallbacks
from src.reporting.console_workflow_callbacks import ConsoleWorkflowCallbacks
from src.reporting.typing import Reporters


# def load_pipeline_reporter_from_list(
def load_pipeline_reporter(
    reporting_dir: str | None,
    reporters: List[Reporters] | None = [],
    index_name: str = "",
    num_workflow_steps: int = 0,
) -> WorkflowCallbacks:
    """Create a callback manager and register a list of reporters.
    Reporters may be configured as generic loggers or associated with a specified indexing job.
    """
    callback_manager = WorkflowCallbacksManager()
    for reporter in reporters:
        match reporter:
            case Reporters.BLOB:
                # create a dedicated container for logs
                container_name = "logs"
                if reporting_dir is not None:
                    container_name = os.path.join(reporting_dir, container_name)
                # ensure the root directory exists; if not, create it
                blob_service_client = BlobServiceClientSingleton.get_instance()
                container_root = Path(container_name).parts[0]
                if not blob_service_client.get_container_client(
                    container_root
                ).exists():
                    blob_service_client.create_container(container_root)
                # register the blob reporter
                callback_manager.register(
                    BlobWorkflowCallbacks(
                        storage_account_blob_url=os.environ["STORAGE_ACCOUNT_BLOB_URL"],
                        container_name=container_name,
                        blob_name=f"{datetime.now().strftime('%Y-%m-%d-%H:%M:%S:%f')}.logs.txt",
                        index_name=index_name,
                        num_workflow_steps=num_workflow_steps,
                    )
                )
            case Reporters.FILE:
                callback_manager.register(FileWorkflowCallbacks(dir=reporting_dir))
            case Reporters.APP_INSIGHTS:
                if os.getenv("APP_INSIGHTS_CONNECTION_STRING"):
                    callback_manager.register(
                        ApplicationInsightsWorkflowCallbacks(
                            connection_string=os.environ[
                                "APP_INSIGHTS_CONNECTION_STRING"
                            ],
                            index_name=index_name,
                            num_workflow_steps=num_workflow_steps,
                        )
                    )
            case Reporters.CONSOLE:
                pass
            case _:
                print(f"WARNING: unknown reporter type: {reporter}. Skipping.")
    # always register the console reporter as a fallback
    callback_manager.register(
        ConsoleWorkflowCallbacks(
            index_name=index_name, num_workflow_steps=num_workflow_steps
        )
    )
    return callback_manager

================
File: backend/src/reporting/pipeline_job_workflow_callbacks.py
================
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT License.

from datashaper.workflow.workflow_callbacks import NoopWorkflowCallbacks

from src.models import PipelineJob
from src.typing import PipelineJobState


class PipelineJobWorkflowCallbacks(NoopWorkflowCallbacks):
    """A reporter that writes to a stream (sys.stdout)."""

    def __init__(self, pipeline_job: "PipelineJob"):
        """
        This class defines a set of callback methods that can be used to report the progress and status of a workflow job.
        It inherits from the NoopWorkflowCallbacks class, which provides default implementations for all the callback methods.

        Attributes:
            pipeline_job (PipelineJob): The pipeline object associated with the job.

        """
        self._pipeline_job = pipeline_job

    def on_workflow_start(self, name: str, instance: object) -> None:
        """Execute this callback when a workflow starts."""
        # if we are not already running, set the status to running
        if self._pipeline_job.status != PipelineJobState.RUNNING:
            self._pipeline_job.status = PipelineJobState.RUNNING
        self._pipeline_job.progress = f"Workflow {name} started."

    def on_workflow_end(self, name: str, instance: object) -> None:
        """Execute this callback when a workflow ends."""
        self._pipeline_job.completed_workflows.append(name)
        self._pipeline_job.update_db()
        self._pipeline_job.progress = f"Workflow {name} complete."
        self._pipeline_job.percent_complete = (
            self._pipeline_job.calculate_percent_complete()
        )

================
File: backend/src/reporting/reporter_singleton.py
================
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT License.

import os
from urllib.parse import urlparse

from datashaper import WorkflowCallbacks

from src.reporting.load_reporter import load_pipeline_reporter
from src.reporting.typing import Reporters


class ReporterSingleton:
    _instance: WorkflowCallbacks = None

    @classmethod
    def get_instance(cls) -> WorkflowCallbacks:
        if cls._instance is None:
            # Set up reporters based on environment variable or defaults
            reporters = []
            for reporter_name in os.getenv(
                "REPORTERS", Reporters.CONSOLE.name.upper()
            ).split(","):
                try:
                    reporters.append(Reporters[reporter_name.upper()])
                except KeyError:
                    raise ValueError(f"Found unknown reporter: {reporter_name}")
            cls._instance = load_pipeline_reporter(
                reporting_dir="", reporters=reporters
            )
        return cls._instance


def _is_valid_url(url: str) -> bool:
    try:
        result = urlparse(url)
        return all([result.scheme, result.netloc])
    except ValueError:
        return False

================
File: backend/src/reporting/typing.py
================
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT License.

import logging
from enum import Enum
from typing import Literal

from graphrag.index.config import (
    PipelineReportingConfig,
    reporting,
)
from pydantic import Field as pydantic_Field


class Reporters(Enum):
    BLOB = (1, "blob")
    CONSOLE = (2, "console")
    FILE = (3, "file")
    APP_INSIGHTS = (4, "app_insights")


class PipelineAppInsightsReportingConfig(
    PipelineReportingConfig[Literal["app_insights"]]
):
    """Represents the ApplicationInsights reporting configuration for the pipeline."""

    type: Literal["app_insights"] = Reporters.APP_INSIGHTS.name.lower()
    """The type of reporting."""

    connection_string: str = pydantic_Field(
        description="The connection string for the App Insights instance.",
        default=None,
    )
    """The connection string for the App Insights instance."""

    logger_name: str = pydantic_Field(
        description="The name for logger instance", default=None
    )
    """The name for logger instance"""

    logger_level: int = pydantic_Field(
        description="The name of the logger. Defaults to None.", default=logging.INFO
    )
    """The name of the logger. Defaults to None."""


# add the new type to the existing PipelineReportingConfigTypes
PipelineReportingConfigTypes = (
    reporting.PipelineReportingConfigTypes | PipelineAppInsightsReportingConfig
)

================
File: backend/src/tests/api/test_azure_clients.py
================
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT License.

from unittest.mock import patch

from azure.cosmos import (
    ContainerProxy,
    CosmosClient,
    DatabaseProxy,
)
from azure.storage.blob import BlobServiceClient
from azure.storage.blob.aio import BlobServiceClient as BlobServiceClientAsync

from src.api.azure_clients import AzureStorageClientManager


class TestAzureStorageClientManager:
    @patch("src.api.azure_clients.BlobServiceClientAsync.from_connection_string")
    @patch("src.api.azure_clients.BlobServiceClient.from_connection_string")
    @patch("src.api.azure_clients.CosmosClient.from_connection_string")
    def get_azure_storage_client_manager(
        self,
        mock_cosmos_client,
        mock_blob_service_client,
        mock_blob_service_client_async,
    ):
        mock_blob_service_client.return_value = BlobServiceClient
        mock_blob_service_client_async.return_value = BlobServiceClientAsync
        mock_cosmos_client.return_value = CosmosClient
        manager = AzureStorageClientManager()
        return manager

    def test_get_blob_service_client(self):
        manager = self.get_azure_storage_client_manager()
        client = manager.get_blob_service_client()
        assert client == BlobServiceClient

    def test_get_blob_service_client_async(self):
        manager = self.get_azure_storage_client_manager()
        client = manager.get_blob_service_client_async()
        assert client == BlobServiceClientAsync

    def test_get_cosmos_client(self):
        manager = self.get_azure_storage_client_manager()
        client = manager.get_cosmos_client()
        assert client == CosmosClient

    @patch("src.api.azure_clients.CosmosClient.get_database_client")
    def test_get_cosmos_database_client(self, mock_get_database_client):
        mock_get_database_client.return_value = DatabaseProxy
        manager = self.get_azure_storage_client_manager()
        db_name = "test_database"
        client = manager.get_cosmos_database_client(db_name)
        assert client == DatabaseProxy

    @patch("src.api.azure_clients.DatabaseProxy.get_container_client")
    @patch("src.api.azure_clients.CosmosClient.get_database_client")
    def test_get_cosmos_container_client(
        self, mock_get_database_client, mock_get_container_client
    ):
        mock_get_database_client.return_value = DatabaseProxy
        mock_get_container_client.return_value = ContainerProxy
        manager = self.get_azure_storage_client_manager()
        database_name = "test_database"
        container_name = "test_container"
        client = manager.get_cosmos_container_client(
            database_name=database_name, container_name=container_name
        )
        assert client == manager._cosmos_client.get_database_client(
            database_name
        ).get_container_client(container_name)

================
File: backend/src/tests/conftest.py
================
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT License.

import inspect
import os
import shutil
import uuid

import pytest
import requests
import wikipedia
from azure.cosmos import CosmosClient
from azure.identity import DefaultAzureCredential
from azure.storage.blob import BlobServiceClient
from dotenv import load_dotenv

load_dotenv()


def _upload_files(blob_service_client, directory, container_name):
    for root, dirs, files in os.walk(directory):
        for file in files:
            local_path = os.path.join(root, file)
            relative_path = os.path.relpath(local_path, directory)
            # replace backslashes with forward slashes (for Windows compatibility)
            relative_path = relative_path.replace("\\", "/")
            # upload file
            blob_client = blob_service_client.get_blob_client(
                blob=relative_path, container=container_name
            )
            with open(local_path, "rb") as data:
                blob_client.upload_blob(
                    data, connection_timeout=120
                )  # increase timeout for large files


@pytest.fixture(scope="session")
def client(request):
    """Return the session base url which is the deployment url authorized with the apim subscription key stored in your .env file"""
    deployment_url = os.environ["DEPLOYMENT_URL"]
    deployment_url = deployment_url.rstrip("/")
    apim_key = os.environ["APIM_SUBSCRIPTION_KEY"]
    session = requests.Session()
    session.headers.update({"Ocp-Apim-Subscription-Key": apim_key})
    session.base_url = deployment_url
    return session


@pytest.fixture()
def prepare_valid_index_data():
    """Prepare valid test data by uploading the result files of a "valid" indexing run to a new blob container."""
    account_url = os.environ["STORAGE_ACCOUNT_BLOB_URL"]
    credential = DefaultAzureCredential()
    blob_service_client = BlobServiceClient(account_url, credential=credential)

    # generate a unique data container name
    container_name = "test-data-" + str(uuid.uuid4())
    container_name = container_name.replace("_", "-").replace(".", "-").lower()[:63]
    blob_service_client.create_container(container_name)
    this_directory = os.path.dirname(
        os.path.abspath(inspect.getfile(inspect.currentframe()))
    )
    # generate a unique index container name
    index_name = "test-index-" + str(uuid.uuid4())
    index_name = index_name.replace("_", "-").replace(".", "-").lower()[:63]
    blob_service_client.create_container(index_name)
    this_directory = os.path.dirname(
        os.path.abspath(inspect.getfile(inspect.currentframe()))
    )
    # use a small amount of sample text to test the data upload endpoint
    page = wikipedia.page("Alaska")
    os.makedirs(f"{this_directory}/data/sample-data", exist_ok=True)
    with open(f"{this_directory}/data/sample-data/sample_text.txt", "w") as f:
        f.write(page.summary)
    _upload_files(
        blob_service_client,
        f"{this_directory}/data/sample-data",
        container_name,
    )
    _upload_files(blob_service_client, f"{this_directory}/data/test-index", index_name)

    endpoint = os.environ["COSMOS_URI_ENDPOINT"]
    credential = DefaultAzureCredential()
    client = CosmosClient(endpoint, credential)

    container_store = "container-store"
    database = client.get_database_client(container_store)
    container_container = database.get_container_client(container_store)

    index_item = {"id": index_name, "type": "index"}
    data_item = {"id": container_name, "type": "data"}
    container_container.create_item(body=index_item)
    container_container.create_item(body=data_item)

    container_store = "jobs"
    database = client.get_database_client(container_store)
    container_jobs = database.get_container_client(container_store)

    index_item = {
        "id": index_name,
        "index_name": index_name,
        "storage_name": container_name,
        "all_workflows": [
            "create_base_text_units",
            "create_final_text_units",
            "create_base_extracted_entities",
            "create_summarized_entities",
            "create_base_entity_graph",
            "create_final_entities",
            "create_final_relationships",
            "create_base_documents",
            "create_base_document_graph",
            "create_final_documents",
            "create_final_communities",
            "create_final_community_reports",
            "create_final_covariates",
            "create_base_entity_nodes",
            "create_base_document_nodes",
            "create_final_nodes",
        ],
        "completed_workflows": [
            "create_base_text_units",
            "create_base_extracted_entities",
            "create_final_covariates",
            "create_summarized_entities",
            "create_base_entity_graph",
            "create_final_entities",
            "create_final_relationships",
            "create_final_communities",
            "create_final_community_reports",
            "create_base_entity_nodes",
            "create_final_text_units",
            "create_base_documents",
            "create_base_document_graph",
            "create_base_document_nodes",
            "create_final_documents",
            "create_final_nodes",
        ],
        "failed_workflows": [],
        "status": "complete",
        "percent_complete": 100,
        "progress": "16 out of 16 workflows completed successfully.",
    }
    container_jobs.create_item(body=index_item)

    yield index_name  # test runs here

    # clean up
    blob_service_client.delete_container(container_name)
    blob_service_client.delete_container(index_name)
    container_container.delete_item(item=container_name, partition_key=container_name)
    container_jobs.delete_item(item=index_name, partition_key=index_name)
    shutil.rmtree(f"{this_directory}/data/sample-data")


@pytest.fixture()
def prepare_invalid_index_data():
    """Prepare valid test data by uploading the result files of a "valid" indexing run to a new blob container."""
    account_url = os.environ["STORAGE_ACCOUNT_BLOB_URL"]
    credential = DefaultAzureCredential()
    blob_service_client = BlobServiceClient(account_url, credential=credential)

    # generate a unique data container name
    container_name = "test-index-pytest-" + str(uuid.uuid4())
    container_name = container_name.replace("_", "-").replace(".", "-").lower()[:63]
    blob_service_client.create_container(container_name)
    this_directory = os.path.dirname(
        os.path.abspath(inspect.getfile(inspect.currentframe()))
    )
    _upload_files(
        blob_service_client, f"{this_directory}/data/test-index", container_name
    )

    blob_client = blob_service_client.get_blob_client(
        container=container_name, blob="embedded_graph.1.graphml"
    )
    blob_client.delete_blob()

    endpoint = os.environ["COSMOS_URI_ENDPOINT"]
    credential = DefaultAzureCredential()
    client = CosmosClient(endpoint, credential)

    container_store = "container-store"
    database = client.get_database_client(container_store)
    container_container = database.get_container_client(container_store)
    index_item = {"id": container_name, "type": "index"}
    container_container.create_item(body=index_item)

    container_store = "jobs"
    database = client.get_database_client(container_store)
    container_jobs = database.get_container_client(container_store)
    index_item = {
        "id": container_name,
        "storage_name": "data1",
        "task_state": "15 steps out of 15 completed.",
        "percent_complete": "100.0",
    }
    container_jobs.create_item(body=index_item)

    yield container_name  # test runs here

    # clean up
    blob_service_client.delete_container(container_name)
    container_container.delete_item(item=container_name, partition_key=container_name)
    container_jobs.delete_item(item=container_name, partition_key=container_name)


# this fixture tests the creation of an entity configuration, not good practice but it's needed for the other tests
@pytest.fixture
def create_entity_configuration(client):
    endpoint = "/pipeline/config"
    entity_configuration_name = str(uuid.uuid4())
    entity_types = ["ORGANIZATION"]
    entity_examples = [
        {
            "Entity_types": "ORGANIZATION",
            "Text": "Arm's (ARM) stock skyrocketed...",
            "Output": '("entity"{tuple_delimiter}ARM{tuple_delimiter}ORGANIZATION{tuple_delimiter}...)',
        }
    ]
    request_data = {
        "entityConfigurationName": entity_configuration_name,
        "entityTypes": entity_types,
        "entityExamples": entity_examples,
    }
    response = client.post(
        url=f"{client.base_url}{endpoint}/entity/types", json=request_data
    )
    assert response.status_code == 200
    yield entity_configuration_name


@pytest.fixture
def data_upload_small(client):
    this_directory = os.path.dirname(
        os.path.abspath(inspect.getfile(inspect.currentframe()))
    )
    # use a small amount of sample text to test the data upload endpoint
    page = wikipedia.page("Alaska")
    with open(f"{this_directory}/data/sample_text.txt", "w") as f:
        f.write(page.summary)

    # test the upload of data
    files = [("files", open(f"{this_directory}/data/sample_text.txt", "rb"))]
    assert len(files) > 0
    blob_container_name = f"test-data-{str(uuid.uuid4())}"
    print(f"Creating blob data container: {blob_container_name}")
    response = client.post(
        url=f"{client.base_url}/data",
        files=files,
        params={"storage_name": blob_container_name},
    )
    assert response.status_code == 200

    yield blob_container_name  # test runs here

    # clean up
    os.remove(f"{this_directory}/data/sample_text.txt")
    print(f"Deleting blob data container: {blob_container_name}")
    response = client.delete(url=f"{client.base_url}/data/{blob_container_name}")
    assert response.status_code == 200

================
File: backend/src/tests/test_all_index_endpoint.py
================
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT License.

import time
import uuid

import pytest

index_endpoint = "/index"


@pytest.fixture
def create_entity_config(client):
    entity_config_name = f"default-{str(uuid.uuid4())}"
    entity_types = ["ORGANIZATION", "GEO", "PERSON"]
    entity_examples = [
        {
            "entity_types": "ORGANIZATION, PERSON",
            "text": "The Fed is scheduled to meet on Tuesday and Wednesday, with the central bank planning to release its latest policy decision on Wednesday at 2:00 p.m. ET, followed by a press conference where Fed Chair Jerome Powell will take questions. Investors expect the Federal Open Market Committee to hold its benchmark interest rate steady in a range of 5.25%-5.5%.",
            "output": '("entity"{tuple_delimiter}FED{tuple_delimiter}ORGANIZATION{tuple_delimiter}The Fed is the Federal Reserve, which will set interest rates on Tuesday and Wednesday)\n{record_delimiter}\n("entity"{tuple_delimiter}JEROME POWELL{tuple_delimiter}PERSON{tuple_delimiter}Jerome Powell is the chair of the Federal Reserve)\n{record_delimiter}\n("entity"{tuple_delimiter}FEDERAL OPEN MARKET COMMITTEE{tuple_delimiter}ORGANIZATION{tuple_delimiter}The Federal Reserve committee makes key decisions about interest rates and the growth of the United States money supply)\n{record_delimiter}\n("relationship"{tuple_delimiter}JEROME POWELL{tuple_delimiter}FED{tuple_delimiter}Jerome Powell is the Chair of the Federal Reserve and will answer questions at a press conference{tuple_delimiter}9)\n{completion_delimiter}',
        },
        {
            "entity_types": "ORGANIZATION",
            "text": "Arm's (ARM) stock skyrocketed in its opening day on the Nasdaq Thursday. But IPO experts warn that the British chipmaker's debut on the public markets isn't indicative of how other newly listed companies may perform.\n\nArm, a formerly public company, was taken private by SoftBank in 2016. The well-established chip designer says it powers 99% of premium smartphones.",
            "output": '("entity"{tuple_delimiter}ARM{tuple_delimiter}ORGANIZATION{tuple_delimiter}Arm is a stock now listed on the Nasdaq which powers 99% of premium smartphones)\n{record_delimiter}\n("entity"{tuple_delimiter}SOFTBANK{tuple_delimiter}ORGANIZATION{tuple_delimiter}SoftBank is a firm that previously owned Arm)\n{record_delimiter}\n("relationship"{tuple_delimiter}ARM{tuple_delimiter}SOFTBANK{tuple_delimiter}SoftBank formerly owned Arm from 2016 until present{tuple_delimiter}5)\n{completion_delimiter}',
        },
        {
            "entity_types": "ORGANIZATION,GEO,PERSON",
            "text": "Five Americans jailed for years in Iran and widely regarded as hostages are on their way home to the United States.\n\nThe last pieces in a controversial swap mediated by Qatar fell into place when $6bn (£4.8bn) of Iranian funds held in South Korea reached banks in Doha.\n\nIt triggered the departure of the four men and one woman in Tehran, who are also Iranian citizens, on a chartered flight to Qatar's capital.\n\nThey were met by senior US officials and are now on their way to Washington.\n\nThe Americans include 51-year-old businessman Siamak Namazi, who has spent nearly eight years in Tehran's notorious Evin prison, as well as businessman Emad Shargi, 59, and environmentalist Morad Tahbaz, 67, who also holds British nationality.",
            "output": '("entity"{tuple_delimiter}IRAN{tuple_delimiter}GEO{tuple_delimiter}Iran held American citizens as hostages)\n{record_delimiter}\n("entity"{tuple_delimiter}UNITED STATES{tuple_delimiter}GEO{tuple_delimiter}Country seeking to release hostages)\n{record_delimiter}\n("entity"{tuple_delimiter}QATAR{tuple_delimiter}GEO{tuple_delimiter}Country that negotiated a swap of money in exchange for hostages)\n{record_delimiter}\n("entity"{tuple_delimiter}SOUTH KOREA{tuple_delimiter}GEO{tuple_delimiter}Country holding funds from Iran)\n{record_delimiter}\n("entity"{tuple_delimiter}TEHRAN{tuple_delimiter}GEO{tuple_delimiter}Capital of Iran where the Iranian hostages were being held)\n{record_delimiter}\n("entity"{tuple_delimiter}DOHA{tuple_delimiter}GEO{tuple_delimiter}Capital city in Qatar)\n{record_delimiter}\n("entity"{tuple_delimiter}WASHINGTON{tuple_delimiter}GEO{tuple_delimiter}Capital city in United States)\n{record_delimiter}\n("entity"{tuple_delimiter}SIAMAK NAMAZI{tuple_delimiter}PERSON{tuple_delimiter}Hostage who spent time in Tehran\'s Evin prison)\n{record_delimiter}\n("entity"{tuple_delimiter}EVIN PRISON{tuple_delimiter}GEO{tuple_delimiter}Notorious prison in Tehran)\n{record_delimiter}\n("entity"{tuple_delimiter}EMAD SHARGI{tuple_delimiter}PERSON{tuple_delimiter}Businessman who was held hostage)\n{record_delimiter}\n("entity"{tuple_delimiter}MORAD TAHBAZ{tuple_delimiter}PERSON{tuple_delimiter}British national and environmentalist who was held hostage)\n{record_delimiter}\n("relationship"{tuple_delimiter}IRAN{tuple_delimiter}UNITED STATES{tuple_delimiter}Iran negotiated a hostage exchange with the United States{tuple_delimiter}2)\n{record_delimiter}\n("relationship"{tuple_delimiter}QATAR{tuple_delimiter}UNITED STATES{tuple_delimiter}Qatar brokered the hostage exchange between Iran and the United States{tuple_delimiter}2)\n{record_delimiter}\n("relationship"{tuple_delimiter}QATAR{tuple_delimiter}IRAN{tuple_delimiter}Qatar brokered the hostage exchange between Iran and the United States{tuple_delimiter}2)\n{record_delimiter}\n("relationship"{tuple_delimiter}SIAMAK NAMAZI{tuple_delimiter}EVIN PRISON{tuple_delimiter}Siamak Namazi was a prisoner at Evin prison{tuple_delimiter}8)\n{record_delimiter}\n("relationship"{tuple_delimiter}SIAMAK NAMAZI{tuple_delimiter}MORAD TAHBAZ{tuple_delimiter}Siamak Namazi and Morad Tahbaz were exchanged in the same hostage release{tuple_delimiter}2)\n{record_delimiter}\n("relationship"{tuple_delimiter}SIAMAK NAMAZI{tuple_delimiter}EMAD SHARGI{tuple_delimiter}Siamak Namazi and Emad Shargi were exchanged in the same hostage release{tuple_delimiter}2)\n{record_delimiter}\n("relationship"{tuple_delimiter}MORAD TAHBAZ{tuple_delimiter}EMAD SHARGI{tuple_delimiter}Morad Tahbaz and Emad Shargi were exchanged in the same hostage release{tuple_delimiter}2)\n{record_delimiter}\n("relationship"{tuple_delimiter}SIAMAK NAMAZI{tuple_delimiter}IRAN{tuple_delimiter}Siamak Namazi was a hostage in Iran{tuple_delimiter}2)\n{record_delimiter}\n("relationship"{tuple_delimiter}MORAD TAHBAZ{tuple_delimiter}IRAN{tuple_delimiter}Morad Tahbaz was a hostage in Iran{tuple_delimiter}2)\n{record_delimiter}\n("relationship"{tuple_delimiter}EMAD SHARGI{tuple_delimiter}IRAN{tuple_delimiter}Emad Shargi was a hostage in Iran{tuple_delimiter}2)\n{completion_delimiter}',
        },
    ]
    request_data = {
        "entity_configuration_name": entity_config_name,
        "entity_types": entity_types,
        "entity_examples": entity_examples,
    }
    response = client.post(
        url=f"{client.base_url}{index_endpoint}/config/entity", json=request_data
    )
    yield (entity_config_name, response)


def test_create_delete_entity_config(client, create_entity_config):
    entity_config_name, response = create_entity_config
    assert response.status_code == 200
    response = client.delete(
        url=f"{client.base_url}{index_endpoint}/{entity_config_name}"
    )
    assert response.status_code == 200


@pytest.fixture
def index_create(client, data_upload_small):
    blob_container_name = data_upload_small
    index_name = f"test-index-{str(uuid.uuid4())}"
    request_data = {
        "storage_name": blob_container_name,
        "index_name": index_name,
    }
    response = client.post(url=f"{client.base_url}{index_endpoint}", json=request_data)
    yield (index_name, response)  # test runs here


def test_index_create(client, index_create):
    index_name, response = index_create
    assert response.status_code == 200
    response = client.delete(url=f"{client.base_url}{index_endpoint}/{index_name}")


@pytest.fixture
def run_indexing(client, index_create):
    index_name, _ = index_create
    print(f"Testing the building of index: {index_name}")
    while True:
        response = client.get(
            url=f"{client.base_url}{index_endpoint}/status/{index_name}"
        )
        assert response.status_code == 200
        percent_complete = response.json().get("percent_complete", None)
        status = response.json().get("status", None)
        print(f"Percent Complete: {percent_complete}")
        print(f"Status: {status}")
        if status == "failed":
            yield (False, index_name)
            break
        if status == "complete":
            print("Indexing Test Passed!")
            # assert True
            yield (True, index_name)
            break
        time.sleep(10)


def test_indexing_end_to_end(client, run_indexing):
    passed, index_name = run_indexing
    assert passed

    query_endpoint = "/query"
    graph_endpoint = "/graph"

    print(f"Got index name: {index_name}")

    request = {"index_name": index_name, "query": "Where is Alabama?"}
    global_response_new = client.post(
        f"{client.base_url}{query_endpoint}/global", json=request
    )
    assert global_response_new.status_code == 200
    print("Passed global search test.")

    global_response = global_response_new.json()
    if global_response["context_data"]["reports"] != []:
        report_id = global_response["context_data"]["reports"][0]["id"]
        response = client.get(
            f"{client.base_url}/source/report/{index_name}/{report_id}"
        )
        assert response.status_code == 200
        print("Passed /source - retrieving report id.")

    local_response = client.post(
        f"{client.base_url}{query_endpoint}/local", json=request
    )
    assert local_response.status_code == 200
    print("Passed local search test.")

    local_response = local_response.json()
    if local_response["context_data"]["entities"] != []:
        entity_id = local_response["context_data"]["entities"][0]["id"]
        entity_source_response = client.get(
            f"{client.base_url}/source/entity/{index_name}/{entity_id}"
        )
        assert entity_source_response.status_code == 200
        print("Passed /source - retrieving entity id.")

        text_unit_info = entity_source_response.json()
        first_unit = text_unit_info["text_units"][0]
        text_unit_source_response = client.get(
            f"{client.base_url}/source/text/{index_name}/{first_unit}"
        )
        assert text_unit_source_response.status_code == 200
        print("Passed /source - retrieving text unit.")

    if local_response["context_data"]["relationships"] != []:
        relationship_id = local_response["context_data"]["relationships"][0]["id"]
        relationship_response = client.get(
            f"{client.base_url}/source/relationship/{index_name}/{relationship_id}"
        )
        assert relationship_response.status_code == 200
        print("Passed /source - retrieving relationship.")

    response = client.get(f"{client.base_url}{graph_endpoint}/graphml/{index_name}")
    assert response.status_code == 200
    print("Passed retrieving graphml file.")

    response = client.get(f"{client.base_url}{graph_endpoint}/stats/{index_name}")
    assert response.status_code == 200
    print("Passed retrieving graph stats.")

    response = client.delete(url=f"{client.base_url}{index_endpoint}/{index_name}")
    assert response.status_code == 200
    print(f"Passed deleting index: {index_name}")

================
File: backend/src/typing/__init__.py
================
from src.typing.pipeline import PipelineJobState

__all__ = ["PipelineJobState"]

================
File: backend/src/typing/pipeline.py
================
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT License.

from enum import Enum


class PipelineJobState(Enum):
    SCHEDULED = "scheduled"
    RUNNING = "running"
    FAILED = "failed"
    COMPLETE = "complete"

================
File: backend/src/utils/graph.py
================
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT License.

import io
from typing import cast

import networkx as nx
import pandas as pd
from datashaper import (
    NoopWorkflowCallbacks,
    WorkflowCallbacks,
)
from graphrag.index import PipelineStorage
from graphrag.index.emit import (
    TableEmitterType,
    create_table_emitters,
)

from src.reporting.console_workflow_callbacks import ConsoleWorkflowCallbacks

workflows_to_concat = [
    "create_base_text_units",
    "create_base_documents",
    "create_final_covariates",
    "create_base_entity_nodes",
]

workflows_to_merge = ["create_base_extracted_entities", *workflows_to_concat]


def merge_nodes(target: nx.Graph, subgraph: nx.Graph):
    """Merge nodes from subgraph into target using the operations defined in node_ops."""
    for node in subgraph.nodes:
        if node not in target.nodes:
            target.add_node(node, **(subgraph.nodes[node] or {}))
        else:
            merge_attributes(target.nodes[node], subgraph.nodes[node])


def merge_edges(target_graph: nx.Graph, subgraph: nx.Graph):
    """Merge edges from subgraph into target using the operations defined in edge_ops."""
    for source, target, edge_data in subgraph.edges(data=True):  # type: ignore
        if not target_graph.has_edge(source, target):
            target_graph.add_edge(source, target, **(edge_data or {}))
        else:
            merge_attributes(target_graph.edges[(source, target)], edge_data)


def merge_attributes(target_item, source_item):
    separator = ","
    for attrib in ["source_id", "description"]:
        target_attrib = target_item.get(attrib, "") or ""
        source_attrib = source_item.get(attrib, "") or ""
        target_item[attrib] = f"{target_attrib}{separator}{source_attrib}"
        if attrib == "source_id":
            target_item[attrib] = separator.join(
                sorted(set(target_item[attrib].split(separator)))
            )


def merge_two_graphml_dataframes(df1, df2):
    mega_graph = nx.Graph()
    G1 = nx.read_graphml(io.BytesIO(df1["entity_graph"][0].encode()))
    G2 = nx.read_graphml(io.BytesIO(df2["entity_graph"][0].encode()))
    for graph in [G1, G2]:
        merge_nodes(mega_graph, graph)
        merge_edges(mega_graph, graph)
    return pd.DataFrame([{"entity_graph": "\n".join(nx.generate_graphml(mega_graph))}])


async def merge_with_graph(
    merge_with_index: str,
    workflow_name: str,
    workflow_index_name: str,
    workflow_df: pd.DataFrame,
    workflow_storage: PipelineStorage,
    merge_with_storage: PipelineStorage,
    reporter: NoopWorkflowCallbacks | None = None,
) -> pd.DataFrame:
    """Execute this callback when a workflow ends."""

    reporter = reporter or ConsoleWorkflowCallbacks()
    if workflow_name in workflows_to_merge:
        reporter.on_log(
            message=(
                f"Starting index merge process. Workflow name: {workflow_name}, "
                + f"Workflow index name: {workflow_index_name}, "
                + f"Existing Index Name: {merge_with_index}."
            )
        )

        # load existing index table
        data = await merge_with_storage.get(f"{workflow_name}.parquet", as_bytes=True)
        existing_df = pd.read_parquet(io.BytesIO(data))

        # validate data tables
        validate_data(workflow_name, workflow_df, existing_df)

        # merge the data tables
        if workflow_name in workflows_to_concat:
            workflow_df = pd.concat((existing_df, workflow_df), axis=0).reset_index(
                drop=True
            )

        # merge the data graphs
        if workflow_name in ["create_base_extracted_entities"]:
            workflow_df = merge_two_graphml_dataframes(existing_df, workflow_df)

        # create a Parquet emitter to save files to storage
        emitter = create_table_emitters(
            [TableEmitterType.Parquet],
            workflow_storage,
            lambda e, s, d: cast(WorkflowCallbacks, reporter).on_error(
                "Error emitting table", e, s, d
            ),
        ).pop()
        # overwrite the workflow data table
        await emitter.emit(workflow_name, workflow_df)

        # log merge process update
        reporter.on_log(
            message=(
                f"Completed index merge process. Workflow name: {workflow_name}, "
                + f"Workflow index name: {workflow_index_name}, "
                + f"Existing Index Name: {merge_with_index}."
            )
        )

        return workflow_df


def validate_data(
    workflow_name: str,
    workflow_df: pd.DataFrame,
    existing_df: pd.DataFrame,
    reporter: NoopWorkflowCallbacks | None = None,
):
    reporter = reporter or ConsoleWorkflowCallbacks()
    if workflow_df is None or workflow_df.empty:
        reporter.on_error(
            f"The {workflow_name} workflow did not produce any output.",
            cause="{name}.parquet table is None or empty.",
        )
        raise ValueError(f"The {workflow_name} workflow did not produce any output.")

    if existing_df is None or existing_df.empty:
        reporter.on_error(
            "The existing index has no data to merge with.",
            cause=f"{workflow_name}.parquet table is None or empty.",
        )
        raise ValueError(f"{workflow_name}.parquet has no data to merge with.")

================
File: backend/src/utils/query.py
================
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT License.

import pandas as pd
from azure.identity import DefaultAzureCredential
from graphrag.query.indexer_adapters import (
    read_indexer_covariates,
    read_indexer_entities,
    read_indexer_relationships,
    read_indexer_reports,
    read_indexer_text_units,
)

from src.api.azure_clients import BlobServiceClientSingleton

storage_options = {
    "account_name": BlobServiceClientSingleton.get_storage_account_name(),
    "account_host": BlobServiceClientSingleton.get_instance().url.split("//")[1],
    "credential": DefaultAzureCredential(),
}


def get_entities(
    entity_table_path: str,
    entity_embedding_table_path: str,
    community_level: int = 0,
) -> pd.DataFrame:
    entity_df = pd.read_parquet(
        entity_table_path,
        storage_options=storage_options,
    )
    entity_embedding_df = pd.read_parquet(
        entity_embedding_table_path,
        storage_options=storage_options,
    )
    return pd.DataFrame(
        read_indexer_entities(entity_df, entity_embedding_df, community_level)
    )


def get_reports(
    entity_table_path: str, community_report_table_path: str, community_level: int
) -> pd.DataFrame:
    entity_df = pd.read_parquet(
        entity_table_path,
        storage_options=storage_options,
    )
    report_df = pd.read_parquet(
        community_report_table_path,
        storage_options=storage_options,
    )
    return pd.DataFrame(read_indexer_reports(report_df, entity_df, community_level))


def get_relationships(relationships_table_path: str) -> pd.DataFrame:
    relationship_df = pd.read_parquet(
        relationships_table_path,
        storage_options=storage_options,
    )
    return pd.DataFrame(read_indexer_relationships(relationship_df))


def get_covariates(covariate_table_path: str) -> pd.DataFrame:
    covariate_df = pd.read_parquet(
        covariate_table_path,
        storage_options=storage_options,
    )
    return pd.DataFrame(read_indexer_covariates(covariate_df))


def get_text_units(text_unit_table_path: str) -> pd.DataFrame:
    text_unit_df = pd.read_parquet(
        text_unit_table_path,
        storage_options=storage_options,
    )
    return pd.DataFrame(read_indexer_text_units(text_unit_df))


def get_df(
    table_path: str,
) -> pd.DataFrame:
    df = pd.read_parquet(
        table_path,
        storage_options=storage_options,
    )
    return df

================
File: backend/src/utils/workflows.py
================
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT License.

from typing import List

from graphrag.index.config import PipelineWorkflowReference


def remove_step_from_workflow(
    workflow: PipelineWorkflowReference, step_names: str | List[str]
) -> List[PipelineWorkflowReference]:
    if isinstance(step_names, str):
        step_names = [step_names]
    return [step for step in workflow.steps if step.get("verb") not in step_names]

================
File: backend/src/main.py
================
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT License.

import os
import traceback
from contextlib import asynccontextmanager

import yaml
from fastapi import (
    FastAPI,
    Request,
    status,
)
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import Response
from kubernetes import (
    client,
    config,
)

from src.api.data import data_route
from src.api.graph import graph_route
from src.api.index import index_route
from src.api.index_configuration import index_configuration_route
from src.api.query import query_route
from src.api.query_streaming import query_streaming_route
from src.api.source import source_route
from src.reporting import ReporterSingleton


async def catch_all_exceptions_middleware(request: Request, call_next):
    """a function to globally catch all exceptions and return a 500 response with the exception message"""
    try:
        return await call_next(request)
    except Exception as e:
        reporter = ReporterSingleton().get_instance()
        reporter.on_error(
            message="Unexpected internal server error",
            cause=e,
            stack=traceback.format_exc(),
        )
        return Response("Unexpected internal server error.", status_code=500)


# deploy a cronjob to manage indexing jobs
@asynccontextmanager
async def lifespan(app: FastAPI):
    # This function is called when the FastAPI application first starts up.
    # To manage multiple graphrag indexing jobs, we deploy a k8s cronjob.
    # This cronjob will act as a job manager that creates/manages the execution of graphrag indexing jobs as they are requested.
    try:
        # Check if the cronjob exists and create it if it does not exist
        config.load_incluster_config()
        # retrieve the running pod spec
        core_v1 = client.CoreV1Api()
        pod_name = os.environ["HOSTNAME"]
        pod = core_v1.read_namespaced_pod(
            name=pod_name, namespace=os.environ["AKS_NAMESPACE"]
        )
        # load the cronjob manifest template and update PLACEHOLDER values with correct values using the pod spec
        with open("indexing-job-manager-template.yaml", "r") as f:
            manifest = yaml.safe_load(f)
        manifest["spec"]["jobTemplate"]["spec"]["template"]["spec"]["containers"][0][
            "image"
        ] = pod.spec.containers[0].image
        manifest["spec"]["jobTemplate"]["spec"]["template"]["spec"][
            "serviceAccountName"
        ] = pod.spec.service_account_name
        # retrieve list of existing cronjobs
        batch_v1 = client.BatchV1Api()
        namespace_cronjobs = batch_v1.list_namespaced_cron_job(
            namespace=os.environ["AKS_NAMESPACE"]
        )
        cronjob_names = [cronjob.metadata.name for cronjob in namespace_cronjobs.items]
        # create cronjob if it does not exist
        if manifest["metadata"]["name"] not in cronjob_names:
            batch_v1.create_namespaced_cron_job(
                namespace=os.environ["AKS_NAMESPACE"], body=manifest
            )
    except Exception as e:
        print("Failed to create graphrag cronjob.")
        reporter = ReporterSingleton().get_instance()
        reporter.on_error(
            message="Failed to create graphrag cronjob",
            cause=str(e),
            stack=traceback.format_exc(),
        )
    yield  # This is where the application starts up.
    # shutdown/garbage collection code goes here


app = FastAPI(
    docs_url="/manpage/docs",
    openapi_url="/manpage/openapi.json",
    title="GraphRAG",
    version=os.getenv("GRAPHRAG_VERSION", "undefined_version"),
    lifespan=lifespan,
)
app.middleware("http")(catch_all_exceptions_middleware)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)
app.include_router(data_route)
app.include_router(index_route)
app.include_router(query_route)
app.include_router(query_streaming_route)
app.include_router(index_configuration_route)
app.include_router(source_route)
app.include_router(graph_route)


# health check endpoint
@app.get(
    "/health",
    summary="API health check",
)
def health_check():
    """Returns a 200 response to indicate the API is healthy."""
    return Response(status_code=status.HTTP_200_OK)

================
File: backend/src/models.py
================
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT License.

from dataclasses import dataclass, field
from time import time
from typing import (
    Any,
    List,
)

from azure.cosmos.exceptions import CosmosHttpResponseError
from pydantic import BaseModel

from src.api.azure_clients import AzureStorageClientManager
from src.api.common import sanitize_name
from src.typing import PipelineJobState


class BaseResponse(BaseModel):
    status: str


class ClaimResponse(BaseModel):
    covariate_type: str
    type: str
    description: str
    subject_id: str
    object_id: str
    source_text: str
    text_unit_id: str
    document_ids: List[str]


class EntityResponse(BaseModel):
    name: str
    description: str
    text_units: list[str]


class GraphRequest(BaseModel):
    index_name: str | List[str]
    query: str
    community_level: int | None = None


class GraphResponse(BaseModel):
    result: Any
    context_data: Any


class GraphDataResponse(BaseModel):
    nodes: int
    edges: int


class IndexNameList(BaseModel):
    index_name: List[str]


class IndexStatusResponse(BaseModel):
    status_code: int
    index_name: str
    storage_name: str
    status: str
    percent_complete: float
    progress: str


class ReportResponse(BaseModel):
    text: str


class RelationshipResponse(BaseModel):
    source: str
    source_id: int
    target: str
    target_id: int
    description: str
    text_units: list[str]


class StorageNameList(BaseModel):
    storage_name: List[str]


class TextUnitResponse(BaseModel):
    text: str
    source_document: str


@dataclass
class PipelineJob:
    _id: str = field(default=None, init=False)
    _epoch_request_time: int = field(default=None, init=False)
    _index_name: str = field(default=None, init=False)
    _human_readable_index_name: str = field(default=None, init=False)
    _sanitized_index_name: str = field(default=None, init=False)
    _human_readable_storage_name: str = field(default=None, init=False)
    _sanitized_storage_name: str = field(default=None, init=False)
    _entity_extraction_prompt: str = field(default=None, init=False)
    _community_report_prompt: str = field(default=None, init=False)
    _summarize_descriptions_prompt: str = field(default=None, init=False)
    _all_workflows: List[str] = field(default_factory=list, init=False)
    _completed_workflows: List[str] = field(default_factory=list, init=False)
    _failed_workflows: List[str] = field(default_factory=list, init=False)
    _status: PipelineJobState = field(default=None, init=False)
    _percent_complete: float = field(default=0, init=False)
    _progress: str = field(default="", init=False)

    @staticmethod
    def _jobs_container():
        azure_storage_client = AzureStorageClientManager()
        return azure_storage_client.get_cosmos_container_client(
            database_name="graphrag", container_name="jobs"
        )

    @classmethod
    def create_item(
        cls,
        id: str,
        human_readable_index_name: str,
        human_readable_storage_name: str,
        entity_extraction_prompt: str | None = None,
        community_report_prompt: str | None = None,
        summarize_descriptions_prompt: str | None = None,
        **kwargs,
    ) -> "PipelineJob":
        """
        This method creates a new instance of the PipelineJob class and adds it to the database.

        Args:
            id (str): The ID of the pipeline job.
            index_name (str): The name of the index.
            storage_name (str): The name of the storage.
            entity_extraction_prompt (str): The entity extraction prompt.
            community_prompt (str): The community prompt.
            summarize_descriptions_prompt (str): The prompt for summarizing descriptions.
            all_workflows (List[str]): List of all workflows.
            completed_workflows (List[str]): List of completed workflows.
            failed_workflows (List[str]): List of failed workflows.
            status (PipelineJobState): The status of the pipeline job.
            percent_complete (float): The percentage of completion.
            progress (str): The progress of the pipeline job.
        Returns:
            PipelineJob: The created pipeline job instance.
        """
        if PipelineJob.item_exist(id):
            raise ValueError(
                f"Pipeline job with ID {id} already exist. "
                "Use PipelineJob.load_item() to create a new pipeline job."
            )

        assert id is not None, "ID cannot be None."
        assert human_readable_index_name is not None, "index_name cannot be None."
        assert len(human_readable_index_name) > 0, "index_name cannot be empty."
        assert human_readable_storage_name is not None, "storage_name cannot be None."
        assert len(human_readable_storage_name) > 0, "storage_name cannot be empty."

        instance = cls.__new__(
            cls, id, human_readable_index_name, human_readable_storage_name, **kwargs
        )
        instance._id = id
        instance._epoch_request_time = int(time())
        instance._human_readable_index_name = human_readable_index_name
        instance._sanitized_index_name = sanitize_name(human_readable_index_name)
        instance._human_readable_storage_name = human_readable_storage_name
        instance._sanitized_storage_name = sanitize_name(human_readable_storage_name)
        instance._entity_extraction_prompt = entity_extraction_prompt
        instance._community_report_prompt = community_report_prompt
        instance._summarize_descriptions_prompt = summarize_descriptions_prompt
        instance._all_workflows = kwargs.get("all_workflows", [])
        instance._completed_workflows = kwargs.get("completed_workflows", [])
        instance._failed_workflows = kwargs.get("failed_workflows", [])
        instance._status = PipelineJobState(
            kwargs.get("status", PipelineJobState.SCHEDULED.value)
        )
        instance._percent_complete = kwargs.get("percent_complete", 0.0)
        instance._progress = kwargs.get("progress", "")

        # Create the item in the database
        instance.update_db()
        return instance

    @classmethod
    def load_item(cls, id: str) -> "PipelineJob":
        """
        This method loads an existing pipeline job from the database and returns
        it as an instance of the PipelineJob class.

        Args:
            id (str): The ID of the pipeline job.

        Returns:
            PipelineJob: The loaded pipeline job instance.
        """
        try:
            db_item = PipelineJob._jobs_container().read_item(item=id, partition_key=id)
        except CosmosHttpResponseError:
            raise ValueError(
                f"Pipeline job with ID {id} does not exist. "
                "Use PipelineJob.create_item() to create a new pipeline job."
            )
        instance = cls.__new__(cls, **db_item)
        instance._id = db_item.get("id")
        instance._epoch_request_time = db_item.get("epoch_request_time")
        instance._index_name = db_item.get("index_name")
        instance._human_readable_index_name = db_item.get("human_readable_index_name")
        instance._sanitized_index_name = db_item.get("sanitized_index_name")
        instance._human_readable_storage_name = db_item.get(
            "human_readable_storage_name"
        )
        instance._sanitized_storage_name = db_item.get("sanitized_storage_name")
        instance._entity_extraction_prompt = db_item.get("entity_extraction_prompt")
        instance._community_report_prompt = db_item.get("community_report_prompt")
        instance._summarize_descriptions_prompt = db_item.get(
            "summarize_descriptions_prompt"
        )
        instance._all_workflows = db_item.get("all_workflows", [])
        instance._completed_workflows = db_item.get("completed_workflows", [])
        instance._failed_workflows = db_item.get("failed_workflows", [])
        instance._status = PipelineJobState(db_item.get("status"))
        instance._percent_complete = db_item.get("percent_complete", 0.0)
        instance._progress = db_item.get("progress", "")
        return instance

    @staticmethod
    def item_exist(id: str) -> bool:
        try:
            PipelineJob._jobs_container().read_item(item=id, partition_key=id)
            return True
        except CosmosHttpResponseError:
            return False

    def calculate_percent_complete(self) -> float:
        """
        This method calculates the percentage of completion of the pipeline job.

        Returns:
            float: The percentage of completion.
        """
        if len(self.completed_workflows) == 0 or len(self.all_workflows) == 0:
            return 0.0
        return round(
            (len(self.completed_workflows) / len(self.all_workflows)) * 100, ndigits=2
        )

    def dump_model(self) -> dict:
        model = {
            "id": self._id,
            "epoch_request_time": self._epoch_request_time,
            "human_readable_index_name": self._human_readable_index_name,
            "sanitized_index_name": self._sanitized_index_name,
            "human_readable_storage_name": self._human_readable_storage_name,
            "sanitized_storage_name": self._sanitized_storage_name,
            "all_workflows": self._all_workflows,
            "completed_workflows": self._completed_workflows,
            "failed_workflows": self._failed_workflows,
            "status": self._status.value,
            "percent_complete": self._percent_complete,
            "progress": self._progress,
        }
        if self._entity_extraction_prompt:
            model["entity_extraction_prompt"] = self._entity_extraction_prompt
        if self._community_report_prompt:
            model["community_report_prompt"] = self._community_report_prompt
        if self._summarize_descriptions_prompt:
            model["summarize_descriptions_prompt"] = self._summarize_descriptions_prompt
        return model

    def update_db(self):
        PipelineJob._jobs_container().upsert_item(body=self.dump_model())

    @property
    def id(self) -> str:
        return self._id

    @id.setter
    def id(self, id: str) -> None:
        if self._id is not None:
            self._id = id
        else:
            raise ValueError("ID cannot be changed once set.")

    @property
    def epoch_request_time(self) -> int:
        return self._epoch_request_time

    @epoch_request_time.setter
    def epoch_request_time(self, epoch_request_time: int) -> None:
        if self._epoch_request_time is not None:
            self._epoch_request_time = epoch_request_time
        else:
            raise ValueError("ID cannot be changed once set.")

    @property
    def human_readable_index_name(self) -> str:
        return self._human_readable_index_name

    @human_readable_index_name.setter
    def human_readable_index_name(self, human_readable_index_name: str) -> None:
        self._human_readable_index_name = human_readable_index_name
        self.update_db()

    @property
    def sanitized_index_name(self) -> str:
        return self._sanitized_index_name

    @sanitized_index_name.setter
    def sanitized_index_name(self, sanitized_index_name: str) -> None:
        self._sanitized_index_name = sanitized_index_name
        self.update_db()
        self._sanitized_storage_name = sanitized_storage_name
        self.update_db()

    @property
    def human_readable_storage_name(self) -> str:
        return self._human_readable_storage_name

    @human_readable_storage_name.setter
    def human_readable_storage_name(self, human_readable_storage_name: str) -> None:
        self._human_readable_storage_name = human_readable_storage_name
        self.update_db()

    @property
    def sanitized_storage_name(self) -> str:
        return self._sanitized_storage_name

    @sanitized_storage_name.setter
    def sanitized_storage_name(self, sanitized_storage_name: str) -> None:
        self._sanitized_storage_name = sanitized_storage_name
        self.update_db()

    @property
    def entity_extraction_prompt(self) -> str:
        return self._entity_extraction_prompt

    @entity_extraction_prompt.setter
    def entity_extraction_prompt(self, entity_extraction_prompt: str) -> None:
        self._entity_extraction_prompt = entity_extraction_prompt
        self.update_db()

    @property
    def community_report_prompt(self) -> str:
        return self._community_report_prompt

    @community_report_prompt.setter
    def community_report_prompt(self, community_report_prompt: str) -> None:
        self._community_report_prompt = community_report_prompt
        self.update_db()

    @property
    def summarize_descriptions_prompt(self) -> str:
        return self._summarize_descriptions_prompt

    @summarize_descriptions_prompt.setter
    def summarize_descriptions_prompt(self, summarize_descriptions_prompt: str) -> None:
        self._summarize_descriptions_prompt = summarize_descriptions_prompt
        self.update_db()

    @property
    def all_workflows(self) -> List[str]:
        return self._all_workflows

    @all_workflows.setter
    def all_workflows(self, all_workflows: List[str]) -> None:
        self._all_workflows = all_workflows
        self.update_db()

    @property
    def completed_workflows(self) -> List[str]:
        return self._completed_workflows

    @completed_workflows.setter
    def completed_workflows(self, completed_workflows: List[str]) -> None:
        self._completed_workflows = completed_workflows
        self.update_db()

    @property
    def failed_workflows(self) -> List[str]:
        return self._failed_workflows

    @failed_workflows.setter
    def failed_workflows(self, failed_workflows: List[str]) -> None:
        self._failed_workflows = failed_workflows
        self.update_db()

    @property
    def status(self) -> PipelineJobState:
        return self._status

    @status.setter
    def status(self, status: PipelineJobState) -> None:
        self._status = status
        self.update_db()

    @property
    def percent_complete(self) -> float:
        return self._percent_complete

    @percent_complete.setter
    def percent_complete(self, percent_complete: float) -> None:
        self._percent_complete = percent_complete
        self.update_db()

    @property
    def progress(self) -> str:
        return self._progress

    @progress.setter
    def progress(self, progress: str) -> None:
        self._progress = progress
        self.update_db()

================
File: backend/manage-indexing-jobs.py
================
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT License.

"""
Note: This script is intended to be executed as a cron job on kubernetes.

A naive implementation of a job manager that leverages k8s CronJob and CosmosDB
to schedule graphrag indexing jobs on a first-come-first-serve basis (based on epoch time).
"""

import os

import pandas as pd
import yaml
from kubernetes import (
    client,
    config,
)

from src.api.azure_clients import AzureStorageClientManager
from src.api.common import sanitize_name
from src.models import PipelineJob
from src.reporting.reporter_singleton import ReporterSingleton
from src.typing.pipeline import PipelineJobState


def schedule_indexing_job(index_name: str):
    """
    Schedule a k8s job to run graphrag indexing for a given index name.
    """
    try:
        config.load_incluster_config()
        # get container image name
        core_v1 = client.CoreV1Api()
        pod_name = os.environ["HOSTNAME"]
        pod = core_v1.read_namespaced_pod(
            name=pod_name, namespace=os.environ["AKS_NAMESPACE"]
        )
        # retrieve job manifest template and replace necessary values
        job_manifest = _generate_aks_job_manifest(
            docker_image_name=pod.spec.containers[0].image,
            index_name=index_name,
            service_account_name=pod.spec.service_account_name,
        )
        batch_v1 = client.BatchV1Api()
        batch_v1.create_namespaced_job(
            body=job_manifest, namespace=os.environ["AKS_NAMESPACE"]
        )
    except Exception:
        reporter = ReporterSingleton().get_instance()
        reporter.on_error(
            "Index job manager encountered error scheduling indexing job",
        )
        # In the event of a catastrophic scheduling failure, something in k8s or the job manifest is likely broken.
        # Set job status to failed to prevent an infinite loop of re-scheduling
        pipelinejob = PipelineJob()
        pipeline_job = pipelinejob.load_item(sanitize_name(index_name))
        pipeline_job["status"] = PipelineJobState.FAILED


def _generate_aks_job_manifest(
    docker_image_name: str,
    index_name: str,
    service_account_name: str,
) -> dict:
    """Generate an AKS Jobs manifest file with the specified parameters.

    The manifest must be valid YAML with certain values replaced by the provided arguments.
    """
    # NOTE: this file location is relative to the WORKDIR set in Dockerfile-backend
    with open("indexing-job-template.yaml", "r") as f:
        manifest = yaml.safe_load(f)
    manifest["metadata"]["name"] = f"indexing-job-{sanitize_name(index_name)}"
    manifest["spec"]["template"]["spec"]["serviceAccountName"] = service_account_name
    manifest["spec"]["template"]["spec"]["containers"][0]["image"] = docker_image_name
    manifest["spec"]["template"]["spec"]["containers"][0]["command"] = [
        "python",
        "run-indexing-job.py",
        f"-i={index_name}",
    ]
    return manifest


def list_k8s_jobs(namespace: str) -> list[str]:
    """List all k8s jobs in a given namespace."""
    config.load_incluster_config()
    batch_v1 = client.BatchV1Api()
    jobs = batch_v1.list_namespaced_job(namespace=namespace)
    job_list = []
    for job in jobs.items:
        job_list.append(job.metadata.name)
    return job_list


def main():
    """
    There are two places to check to determine if an indexing job should be executed:
        * Kubernetes: check if there are any active k8s jobs running in the cluster
        * CosmosDB: check if there are any indexing jobs in a scheduled state

    Ideally if an indexing job has finished or failed, the job status will be reflected in cosmosdb.
    However, if an indexing job failed due to OOM, the job status will not have been updated in cosmosdb.

    To avoid a catastrophic failure scenario where all indexing jobs are stuck in a scheduled state,
    both checks are necessary.
    """
    kubernetes_jobs = list_k8s_jobs(os.environ["AKS_NAMESPACE"])

    azure_storage_client_manager = AzureStorageClientManager()
    job_container_store_client = (
        azure_storage_client_manager.get_cosmos_container_client(
            database_name="graphrag", container_name="jobs"
        )
    )
    # retrieve status of all index jobs that are scheduled or running
    job_metadata = []
    for item in job_container_store_client.read_all_items():
        if item["status"] == PipelineJobState.RUNNING.value:
            # if index job has running state but no associated k8s job, a catastrophic
            # failure (OOM for example) occurred. Set job status to failed.
            if len(kubernetes_jobs) == 0:
                print(
                    f"Indexing job for '{item['human_readable_index_name']}' in 'running' state but no associated k8s job found. Updating to failed state."
                )
                pipelinejob = PipelineJob()
                pipeline_job = pipelinejob.load_item(item["sanitized_index_name"])
                pipeline_job["status"] = PipelineJobState.FAILED.value
            else:
                print(
                    f"Indexing job for '{item['human_readable_index_name']}' already running. Will not schedule another. Exiting..."
                )
                exit()
        if item["status"] == PipelineJobState.SCHEDULED.value:
            job_metadata.append({
                "human_readable_index_name": item["human_readable_index_name"],
                "epoch_request_time": item["epoch_request_time"],
                "status": item["status"],
                "percent_complete": item["percent_complete"],
            })

    # exit if no 'scheduled' jobs were found
    if not job_metadata:
        print("No jobs found")
        exit()
    # convert to dataframe for easier processing
    df = pd.DataFrame(job_metadata)
    # jobs should be run in the order they were requested - sort by epoch_request_time
    df.sort_values(by="epoch_request_time", ascending=True, inplace=True)
    index_to_schedule = df.iloc[0]["human_readable_index_name"]
    print(f"Scheduling job for index: {index_to_schedule}")
    schedule_indexing_job(index_to_schedule)


if __name__ == "__main__":
    main()

================
File: backend/run-indexing-job.py
================
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT License.

import argparse
import asyncio

from src import main  # noqa: F401
from src.api.index import _start_indexing_pipeline

parser = argparse.ArgumentParser(description="Kickoff indexing job.")
parser.add_argument("-i", "--index-name", required=True)
args = parser.parse_args()

asyncio.run(
    _start_indexing_pipeline(
        index_name=args.index_name,
    )
)
